{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing Automatically Discovered Circuits\n",
    "\n",
    "Procedure: \n",
    "- Compute prune scores (via attribution patching) \n",
    "- Search over different thresholds to find the smallest circuit where the null hypotheis of Equivalence / Dominance cannot be rejected \n",
    "- Prune edges from circuit that are not in paths to the output, or in the case of resample ablation cannot be reached from the input\n",
    "- Test whether each edge in the circuit is minimal \n",
    "- Test whether the circuit is complete (by seeing if the null hypothesis on the independence test can be rejected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Dict, Tuple, Union, Optional, Any, Literal, NamedTuple\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from scipy.stats import binom, beta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "from auto_circuit.data import PromptPairBatch, PromptDataLoader\n",
    "from auto_circuit.utils.patchable_model import PatchableModel\n",
    "from auto_circuit.types import (BatchKey, PruneScores, CircuitOutputs, AblationType, \n",
    "                                Edge, BatchOutputs, EdgeCounts)\n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.prune_algos.activation_patching import act_patch_prune_scores\n",
    "from auto_circuit.prune_algos.ACDC import acdc_prune_scores\n",
    "from auto_circuit.prune_algos.subnetwork_probing import subnetwork_probing_prune_scores\n",
    "from auto_circuit.prune_algos.circuit_probing import circuit_probing_prune_scores\n",
    "from auto_circuit.visualize import draw_seq_graph\n",
    "from auto_circuit.utils.custom_tqdm import tqdm\n",
    "from auto_circuit.utils.tensor_ops import desc_prune_scores\n",
    "from auto_circuit.utils.graph_utils import edge_counts_util\n",
    "\n",
    "from auto_circuit_tests.prune_algos import PruneAlgo\n",
    "from auto_circuit_tests.score_funcs import GradFunc, AnswerFunc, DIV_ANSWER_FUNCS\n",
    "# from auto_circuit_tests.faithful_metrics import FaithfulMetric\n",
    "\n",
    "from auto_circuit_tests.utils.auto_circuit_utils import (\n",
    "    run_circuit_with_edges_ablated, \n",
    "    run_fully_ablated_model, \n",
    "    flat_prune_scores_ordered\n",
    ")\n",
    "\n",
    "from auto_circuit_tests.faith_metrics import compute_faith_metrics\n",
    "from auto_circuit_tests.hypo_tests.equiv_test import equiv_tests\n",
    "from auto_circuit_tests.hypo_tests.minimality_test import (\n",
    "    run_circuits_inflated_ablated, \n",
    "    score_diffs,\n",
    "    minimality_test_edge,\n",
    "    minimality_test, \n",
    ")\n",
    "from auto_circuit_tests.hypo_tests.indep_test import independence_tests, indep_test\n",
    "from auto_circuit_tests.hypo_tests.utils import (\n",
    "    join_values, \n",
    "    remove_el,\n",
    "    edges_from_mask, \n",
    "    result_to_json, \n",
    ")\n",
    "from auto_circuit_tests.edge_graph import (\n",
    "    SeqGraph,  \n",
    "    sample_paths, \n",
    "    SampleType,\n",
    "    edge_in_path, \n",
    "    find_unused_edges,\n",
    "    visualize_graph\n",
    ")\n",
    "\n",
    "from auto_circuit_tests.tasks import TASK_DICT, TASK_TO_OUTPUT_ANSWER_FUNCS\n",
    "from auto_circuit_tests.utils.auto_circuit_utils import edge_name\n",
    "from auto_circuit_tests.utils.utils import (\n",
    "    repo_path_to_abs_path, \n",
    "    load_cache, \n",
    "    save_cache, \n",
    "    save_json, \n",
    "    load_json, # should probably move this to auto_circuit_tests.utils\n",
    "    get_el_rank\n",
    ")\n",
    "from auto_circuit_tests.utils.utils import get_exp_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class\n",
    "from dataclasses import dataclass, field\n",
    "@dataclass \n",
    "class Config: \n",
    "    task: str = \"Docstring Component Circuit\" # check how many edges in component circuit (probably do all but ioi toen)\n",
    "    ablation_type: AblationType = AblationType.RESAMPLE\n",
    "    grad_func: GradFunc = GradFunc.LOGIT\n",
    "    answer_func: AnswerFunc = AnswerFunc.MAX_DIFF\n",
    "    eval_grad_func: Optional[GradFunc] = None # TODO: used to evaluate faithfulness\n",
    "    prune_algo: PruneAlgo = PruneAlgo.ACT_PATCH\n",
    "    eval_answer_func: Optional[AnswerFunc] = None\n",
    "    ig_samples: Optional[int] = None\n",
    "    layerwise: bool = False\n",
    "    edge_counts: EdgeCounts = EdgeCounts.LOGARITHMIC\n",
    "    tao_bases: list[float] = field(default_factory=lambda: [1, 5])\n",
    "    tao_exps: list[float] = field(default_factory=lambda: list(range(-5, -1)))\n",
    "    prune_score_thresh: bool = False\n",
    "    null_good: bool = True\n",
    "    alpha: float = 0.05\n",
    "    epsilon: Optional[float] = 0.1\n",
    "    q_star: float = 0.9 \n",
    "    n_paths: int = 200\n",
    "    sample_type: SampleType = SampleType.RANDOM_WALK\n",
    "    # TODO: remove these?\n",
    "    min_equiv_all_edges_thresh = 1000\n",
    "    max_edges_to_test_in_order: int = 0 #TODO: change to 125\n",
    "    max_edges_to_test_without_fail: int = 500 #TODO: change to 125\n",
    "    save_cache: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # always override clean_corrupt for now\n",
    "        self.clean_corrupt = \"corrupt\" if self.ablation_type == AblationType.RESAMPLE else None\n",
    "\n",
    "        # set eval_grad_func and faith_answer_func to logit and max diff if answer_func is div\n",
    "        if self.answer_func in DIV_ANSWER_FUNCS: \n",
    "            if self.eval_grad_func is None:\n",
    "                self.eval_grad_func = GradFunc.LOGIT\n",
    "            if self.eval_answer_func is None:\n",
    "                self.eval_answer_func = AnswerFunc.MAX_DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config \n",
    "conf = Config()\n",
    "#get config overrides if runnign from command line\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), OmegaConf.from_cli(sys.argv[1:]))\n",
    "    conf = Config(**conf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle directories\n",
    "task_dir, ablation_dir, out_answer_dir, ps_dir, edge_dir, exp_dir = get_exp_dir(\n",
    "    task_key=conf.task, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    grad_func=conf.grad_func,\n",
    "    answer_func=conf.answer_func,\n",
    "    prune_algo=conf.prune_algo,\n",
    "    ig_samples=conf.ig_samples,\n",
    "    layerwise=conf.layerwise,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.epsilon,\n",
    "    q_star=conf.q_star,\n",
    "    prune_score_thresh=conf.prune_score_thresh,\n",
    ")\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize task\n",
    "task = TASK_DICT[conf.task]\n",
    "task.shuffle = False\n",
    "task.init_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACDC Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ACDC:\n",
    "    auto_ps_path = out_answer_dir / \"auto_prune_scores.pkl\"\n",
    "    if auto_ps_path.exists():\n",
    "        auto_prune_scores = torch.load(auto_ps_path)\n",
    "    else:\n",
    "        if conf.task == \"Indirect Object Identification Token Circuit\":\n",
    "            print(\"cannot run ACDC on IOI token circuit, too large\")\n",
    "            exit()\n",
    "        auto_prune_scores = acdc_prune_scores(\n",
    "            model=task.model, \n",
    "            dataloader=task.train_loader, \n",
    "            official_edges=None,\n",
    "            tao_exps=conf.tao_exps,\n",
    "            tao_bases=conf.tao_bases,\n",
    "            faithfulness_target=conf.answer_func.value, \n",
    "        )\n",
    "        if conf.save_cache:\n",
    "            torch.save(auto_prune_scores, auto_ps_path)\n",
    "    taos = sorted([tao_base * 10**tao_exp for tao_base, tao_exp in product(conf.tao_bases, conf.tao_exps)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Probing Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.CIRC_PROBE and conf.answer_func == AnswerFunc.KL_DIV:\n",
    "    circ_probe_ps_path = out_answer_dir / \"circ_probe_prune_scores.pkl\"\n",
    "    if circ_probe_ps_path.exists():\n",
    "        circ_probe_prune_scores = torch.load(circ_probe_ps_path)\n",
    "    else:\n",
    "        circ_probe_prune_scores = circuit_probing_prune_scores(\n",
    "            model=task.model, \n",
    "            dataloader=task.train_loader, \n",
    "            official_edges=None,\n",
    "            tree_optimisation=True,\n",
    "            faithfulness_target=conf.answer_func.value, \n",
    "            circuit_sizes=[10, 100],#edge_counts_util(task.model.edges, conf.edge_counts, all_edges=False),\n",
    "            learning_rate=0.1,\n",
    "            epochs=100, \n",
    "            regularize_lambda=0.1, \n",
    "            show_train_graph=False\n",
    "        )\n",
    "        if conf.save_cache:\n",
    "            torch.save(circ_probe_prune_scores, circ_probe_ps_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Patching Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from cache if exists \n",
    "act_ps_path = out_answer_dir / \"act_patch_prune_scores.pt\"\n",
    "if act_ps_path.exists():\n",
    "    act_prune_scores = torch.load(act_ps_path)\n",
    "    act_prune_scores = {mod_name: score for mod_name, score in act_prune_scores.items()} # negative b/c high score should imply large drop in performance\n",
    "else:\n",
    "    act_prune_scores = None\n",
    "\n",
    "# # if act_patch and act_patch doesn't exist, exit\n",
    "# if conf.prune_algo == PruneAlgo.ACT_PATCH and act_prune_scores is None:\n",
    "#     print(\"act_patch_prune_scores.pkl not found, exiting\")\n",
    "#     exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Attribution Patching Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH:\n",
    "    attr_ps_name = \"attrib_patch_prune_scores\"\n",
    "    attr_ps_path = (ps_dir / attr_ps_name).with_suffix(\".pt\")\n",
    "    # if (attr_ps_path).exists():\n",
    "    #     attr_prune_scores = torch.load(attr_ps_path)\n",
    "    # else: \n",
    "    max_layer = max([edge.src.layer for edge in task.model.edges])\n",
    "\n",
    "    attr_prune_scores = mask_gradient_prune_scores(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader,\n",
    "        official_edges=None,\n",
    "        grad_function=conf.grad_func.value, \n",
    "        answer_function=conf.answer_func.value, #answer_func,\n",
    "        mask_val=0.0 if conf.ig_samples is None else None, \n",
    "        ablation_type=conf.ablation_type,\n",
    "        integrated_grad_samples=conf.ig_samples, \n",
    "        layers=max_layer if conf.layerwise else None,\n",
    "        clean_corrupt=conf.clean_corrupt,\n",
    "    )\n",
    "    if conf.save_cache:\n",
    "        torch.save(attr_prune_scores, attr_ps_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare Activation and Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH:\n",
    "    # order = sorted(list(act_prune_scores.keys()), key=lambda x: int(x.split('.')[1]))\n",
    "    order = list(act_prune_scores.keys())\n",
    "    act_prune_scores_flat = flat_prune_scores_ordered(act_prune_scores, order=order)\n",
    "    attr_prune_scores_flat = flat_prune_scores_ordered(attr_prune_scores, order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse and median se\n",
    "if conf.prune_algo != PruneAlgo.ACT_PATCH and act_prune_scores is not None:\n",
    "    mse_result_name = \"act_attr_mse\"\n",
    "    mse_result_path = (ps_dir / mse_result_name).with_suffix(\".json\")\n",
    "    if mse_result_path.exists():\n",
    "        mse_result = load_json(ps_dir, mse_result_name + '.json')\n",
    "    else:\n",
    "        prune_score_diffs = [\n",
    "            (act_prune_scores[mod_name] - attr_prune_scores[mod_name]).flatten()\n",
    "            for mod_name, _patch_mask in task.model.patch_masks.items()\n",
    "        ]\n",
    "        sq_error = torch.concat(prune_score_diffs).pow(2)\n",
    "        median_se = sq_error.median()\n",
    "        mean_se = sq_error.mean()\n",
    "        mse_result = {\n",
    "            \"median_se\": median_se.item(),\n",
    "            \"mean_se\": mean_se.item(),\n",
    "        }\n",
    "        save_json(mse_result, ps_dir, mse_result_name)\n",
    "    print(mse_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH and act_prune_scores is not None:\n",
    "    from scipy import stats \n",
    "    abs_corr, abs_p_value = stats.spearmanr(act_prune_scores_flat.abs().cpu(), attr_prune_scores_flat.abs().cpu())\n",
    "    corr, p_value = stats.spearmanr(act_prune_scores_flat.cpu(), attr_prune_scores_flat.cpu())\n",
    "    print(f\"abs corr: {abs_corr}, abs p-value: {abs_p_value}\")\n",
    "    print(f\"corr: {corr}, p-value: {p_value}\")\n",
    "\n",
    "    spearman_results = {\n",
    "        \"abs_corr\": abs_corr,\n",
    "        \"abs_p_value\": abs_p_value,\n",
    "        \"corr\": corr,\n",
    "        \"p_value\": p_value,\n",
    "    }\n",
    "    save_json(spearman_results, ps_dir, \"spearman_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rank for scores\n",
    "if conf.prune_algo != PruneAlgo.ACT_PATCH:\n",
    "    act_prune_scores_rank = get_el_rank(act_prune_scores_flat.cpu())\n",
    "    attr_prune_scores_rank = get_el_rank(attr_prune_scores_flat.cpu())\n",
    "\n",
    "    act_prune_scores_0 = (act_prune_scores_flat == 0).cpu()\n",
    "    act_prune_scores_0_rank = act_prune_scores_rank[act_prune_scores_0]\n",
    "    min_0_rank, max_0_rank = act_prune_scores_0_rank.min().item(), act_prune_scores_0_rank.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH:\n",
    "    # TODO: plot x=0\n",
    "    plt.scatter(act_prune_scores_rank, attr_prune_scores_rank, s=0.1)\n",
    "    # plot min rank, max rank as vertical lines\n",
    "    plt.axvline(min_0_rank, color='blue', linestyle='--')\n",
    "    plt.axvline(max_0_rank, color='blue', linestyle='--')\n",
    "    # shade area between min and max rank\n",
    "    plt.axvspan(min_0_rank, max_0_rank, color='lightblue', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel(\"Act Patch Rank\")\n",
    "    plt.ylabel(\"Attrib Patch Rank\")\n",
    "    plt.title(\"Rank Correlation\")\n",
    "\n",
    "    plt.savefig(ps_dir / \"rank_corr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I think there must be a bug? \n",
    "# get rank for scores\n",
    "if conf.prune_algo != PruneAlgo.ACT_PATCH:\n",
    "    act_prune_scores_abs_rank = get_el_rank(act_prune_scores_flat.abs().cpu())\n",
    "    attr_prune_scores_abs_rank = get_el_rank(attr_prune_scores_flat.abs().cpu())\n",
    "\n",
    "    max_0_rank = act_prune_scores_abs_rank[act_prune_scores_0].max().item()\n",
    "\n",
    "    plt.scatter(act_prune_scores_abs_rank, attr_prune_scores_abs_rank, s=0.1)\n",
    "    # plot max rank as vertical lines\n",
    "    plt.axvline(max_0_rank, color='blue', linestyle='--')\n",
    "    # shade area between min and max rank\n",
    "    plt.axvspan(0, max_0_rank, color='lightblue', alpha=0.5)\n",
    "\n",
    "    \n",
    "    plt.xlabel(\"Act Patch Rank\")\n",
    "    plt.ylabel(\"Attrib Patch Rank\")\n",
    "    plt.title(\"Rank Correlation Abs\")\n",
    "    plt.savefig(ps_dir / \"rank_corr_abs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Fraction of \"Mis-Signed\" Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH and act_prune_scores is not None:\n",
    "    num_missigned = (act_prune_scores_flat.sign() != attr_prune_scores_flat.sign()).sum()\n",
    "    frac_missigned = num_missigned / len(act_prune_scores_flat)\n",
    "    print(f\"Fraction of missigned: {frac_missigned}\")\n",
    "    save_json({\"frac_missigned\": frac_missigned.item()}, ps_dir, \"missigned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Fraction of Edges Recovered for Each Edge Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for different edge thresholds, compute fraction of edges not included in top k\n",
    "if conf.prune_algo != PruneAlgo.ACT_PATCH:\n",
    "    edge_counts = edge_counts_util(task.model.edges, conf.edge_counts, zero_edges=True)\n",
    "\n",
    "    frac_edges_recovered: Dict[int, float] = {}\n",
    "\n",
    "    for edge_count in edge_counts:\n",
    "        # get indicies where act_prune_scores_abs_rank >= task.n_edges - edge_count\n",
    "        act_indices = act_prune_scores_abs_rank >= task.model.n_edges - edge_count\n",
    "        frac_edges_recovered[edge_count] = (attr_prune_scores_abs_rank[act_indices] >= task.model.n_edges - edge_count).to(t.float).mean().item()\n",
    "\n",
    "    save_json(frac_edges_recovered, ps_dir, \"frac_edges_recovered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parition by Dest Component\n",
    "\n",
    "We partion by Dest B/c we expect difficulties to arise from estimating effects that route through non-linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit_tests.edge_graph import NodeType\n",
    "\n",
    "def mod_name_to_layer_and_node_type(mod_name: str) -> Tuple[int, NodeType]:\n",
    "    _blocks, layer, node_type_str = mod_name.split('.')\n",
    "    layer = int(layer)\n",
    "    if node_type_str == \"hook_k_input\":\n",
    "        node_type = NodeType.K \n",
    "    elif node_type_str == \"hook_q_input\":\n",
    "        node_type = NodeType.Q\n",
    "    elif node_type_str == \"hook_v_input\":\n",
    "        node_type = NodeType.V\n",
    "    elif node_type_str == \"hook_resid_post\":\n",
    "        node_type = NodeType.RESID_END \n",
    "    elif node_type_str == \"hook_mlp_in\":\n",
    "        node_type = NodeType.MLP\n",
    "    else: \n",
    "        raise ValueError(f\"Unknown node type: {node_type_str}\")\n",
    "    return layer, node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH:\n",
    "    from auto_circuit_tests.edge_graph import NodeType\n",
    "    # compute ranking by flatten by order, including module name \n",
    "    def prune_score_rankings_by_component(\n",
    "        prune_scores: PruneScores, \n",
    "        prune_scores_rank: torch.Tensor, \n",
    "        order: list[str]\n",
    "    ) -> dict[tuple[int, NodeType], list[int]]:\n",
    "        # collect mod_name ranking tuples\n",
    "        flat_mod_names = [] \n",
    "        for mod_name in order:\n",
    "            flat_mod_names.extend([mod_name for _ in range(prune_scores[mod_name].numel())])\n",
    "        # get ranking by component type and layer\n",
    "        rank_by_component: dict[tuple[int, NodeType], list[int]] = defaultdict(list)\n",
    "        for mod_name, rank in zip(flat_mod_names, prune_scores_rank):\n",
    "            layer, node_type = mod_name_to_layer_and_node_type(mod_name)\n",
    "            rank_by_component[(layer, node_type)].append(rank)\n",
    "        return rank_by_component\n",
    "\n",
    "    act_rank_by_component = prune_score_rankings_by_component(act_prune_scores, act_prune_scores_abs_rank, order)\n",
    "    attr_rank_by_component = prune_score_rankings_by_component(attr_prune_scores, attr_prune_scores_abs_rank, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # plot ranks for each component type all in one figure\n",
    "    n_layers = max([layer for layer, _ in act_rank_by_component.keys()])\n",
    "    components = sorted(list(set([node_type for _, node_type in act_rank_by_component.keys()])), key=lambda x: x.value)\n",
    "\n",
    "    # Create a 2D array to store the Axes objects\n",
    "    axs = np.empty((len(components), n_layers + 1), dtype=object)\n",
    "\n",
    "    # Create the figure without subplots initially\n",
    "    fig = plt.figure(figsize=(3 * (n_layers+1), 3 * len(components)))\n",
    "\n",
    "\n",
    "    rank_correlations: dict[tuple[int, NodeType], float] = {}\n",
    "    for layer in range(0, n_layers + 1):\n",
    "        for i, node_type in enumerate(components):\n",
    "            act_ranks = act_rank_by_component[(layer, node_type)]\n",
    "            attr_ranks = attr_rank_by_component[(layer, node_type)]\n",
    "            \n",
    "            if len(act_ranks) == 0 and len(attr_ranks) == 0:\n",
    "                continue\n",
    "\n",
    "            # compute rank correlation\n",
    "            corr, p_value = stats.spearmanr(act_ranks, attr_ranks)\n",
    "            rank_correlations[(layer, node_type)] = corr\n",
    "            \n",
    "            # Create a subplot only if there's data to plot\n",
    "            ax = fig.add_subplot(len(components), (n_layers+1), (i * (n_layers+1)) + layer+1)\n",
    "            ax.scatter(act_ranks, attr_ranks, s=1)\n",
    "            # set title below scatter plot\n",
    "            ax.set_title(f\"Correlation: {corr:.2f}\", y=-0.20)\n",
    "\n",
    "            \n",
    "            # Store the Axes object in our 2D array\n",
    "            axs[i, layer - 1] = ax\n",
    "\n",
    "            # Add x-label at the top\n",
    "            if i == 0:\n",
    "                ax.xaxis.set_label_position('top')\n",
    "                ax.set_xlabel(f\"Layer {layer}\", fontweight='bold')\n",
    "            \n",
    "            # Add y-label on the left\n",
    "            if layer == 0 or node_type == NodeType.RESID_END:\n",
    "                ax.set_ylabel(str(node_type.name), fontweight='bold')\n",
    "\n",
    "    # Remove empty spaces in the figure\n",
    "    fig.tight_layout()\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "    # save figure\n",
    "\n",
    "    plt.savefig(ps_dir / \"rank_corr_by_component.png\")\n",
    "\n",
    "\n",
    "    # save rank correlations\n",
    "    save_json({str(k): v for k, v in rank_correlations.items()}, ps_dir, \"rank_cor_by_component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH and act_prune_scores is not None:\n",
    "    # plot scores on x, y\n",
    "    plt.scatter(act_prune_scores_flat.cpu(), attr_prune_scores_flat.cpu(), alpha=0.25)\n",
    "    plt.xlabel(\"Act Patch Scores\")\n",
    "    plt.ylabel(\"Attrib Patch Scores\")\n",
    "    plt.xscale(\"symlog\")\n",
    "    plt.yscale(\"symlog\")\n",
    "    plt.savefig(ps_dir / \"act_attr_scores.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo != PruneAlgo.ACT_PATCH and act_prune_scores is not None:\n",
    "    # plot scores on x, y\n",
    "    plt.scatter(act_prune_scores_flat.abs().cpu(), attr_prune_scores_flat.abs().cpu(), alpha=0.25)\n",
    "    plt.xlabel(\"Act Patch Scores\")\n",
    "    plt.ylabel(\"Attrib Patch Scores\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.savefig(ps_dir / \"act_attr_abs_scores.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construt Circuits from Prune Scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing circuits from prune scores using either edge or fraction of prune score thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prune scores\n",
    "if conf.prune_algo == PruneAlgo.ACDC:\n",
    "    prune_scores = auto_prune_scores\n",
    "elif conf.prune_algo == PruneAlgo.CIRC_PROBE:\n",
    "    prune_scores = circ_probe_prune_scores\n",
    "elif conf.prune_algo == PruneAlgo.ACT_PATCH:\n",
    "    prune_scores = act_prune_scores\n",
    "else:\n",
    "    prune_scores = attr_prune_scores\n",
    "\n",
    "prune_scores = {mod_name: score.to(task.device) for mod_name, score in prune_scores.items()}\n",
    "# sort prune scores\n",
    "sorted_prune_scores = desc_prune_scores(prune_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prune scores\n",
    "def plot_prune_scores(edge_scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    # plot edge scores with x labels max to 0 \n",
    "    ax.plot(sorted(edge_scores, reverse=True))\n",
    "    ax.set_xlim(len(edge_scores), 0)\n",
    "    # log axis \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(\"Edge Count\")\n",
    "    ax.set_ylabel(\"Edge Score\")\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_prune_scores(sorted_prune_scores.cpu().numpy().tolist())\n",
    "plt.savefig(exp_dir / \"edge_scores.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute n_edges \n",
    "from auto_circuit.utils.graph_utils import edge_counts_util\n",
    "circ_edges = []\n",
    "if conf.prune_algo == PruneAlgo.ACDC:\n",
    "     circ_edges = [\n",
    "         t.sum(t.cat([t.flatten(v) for v in auto_prune_scores.values()]) > tao).item()\n",
    "        for tao in taos\n",
    "    ]\n",
    "     circ_thresholds = taos[1:] + [t.inf] # prune scores set to tau if change less than tau\n",
    "else:\n",
    "    circ_edges = edge_counts_util(task.model.edges, conf.edge_counts, zero_edges=True)\n",
    "    circ_thresholds = [sorted_prune_scores[n_edges-1].item() for n_edges in circ_edges]\n",
    "\n",
    "save_json(circ_edges, edge_dir, \"n_circ_edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faithfulness: % Loss Recovered and Equivalence Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use equivalence test from Shi et al to test a. if not equivalent and b. if equivalent\n",
    "\n",
    "We also compute: \n",
    "- mean absolute error: E[abs(score(M) - score(C))] \n",
    "(spiritually similar to Transfomer Circuits not Robust and this comment https://www.lesswrong.com/posts/kcZZAsEjwrbczxN2i/causal-scrubbing-appendix#hJoCMcgXpk8jBLvb7, we don't do fraction of recovered b/c the negatives are weird and annoying)\n",
    "- mean difference: E[score(M)] - E[score(C)] \n",
    "(kind of a middle ground, measuring bias)\n",
    "- frac mean difference recovered: E[score(C)] - E[score(A)] / E[score(M)] - E[score(A)] \n",
    "(SAE work, similar to causal scrubbing, don't need to worry about variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first full model outt and ablated model out\n",
    "\n",
    "with t.inference_mode():\n",
    "    model_out_train: BatchOutputs = {\n",
    "        batch.key: task.model(batch.clean)[task.model.out_slice] \n",
    "        for batch in task.train_loader\n",
    "    }\n",
    "    model_out_test: BatchOutputs = {\n",
    "        batch.key: task.model(batch.clean)[task.model.out_slice] \n",
    "        for batch in task.test_loader\n",
    "    }\n",
    "\n",
    "ablated_out_train: BatchOutputs = run_fully_ablated_model(\n",
    "    model=task.model,\n",
    "    dataloader=task.train_loader,\n",
    "    ablation_type=conf.ablation_type,\n",
    ")\n",
    "\n",
    "ablated_out_test: BatchOutputs = run_fully_ablated_model(\n",
    "    model=task.model,\n",
    "    dataloader=task.test_loader,\n",
    "    ablation_type=conf.ablation_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next get circuit outs for each threshold\n",
    "from auto_circuit.prune import run_circuits\n",
    "from auto_circuit.types import CircuitOutputs, PatchType\n",
    "circuit_outs_train: CircuitOutputs = run_circuits(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    test_edge_counts=circ_edges,\n",
    "    patch_type=PatchType.TREE_PATCH, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    reverse_clean_corrupt=False, \n",
    ")\n",
    "\n",
    "circuit_outs_test: CircuitOutputs = run_circuits(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    test_edge_counts=circ_edges,\n",
    "    patch_type=PatchType.TREE_PATCH, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    reverse_clean_corrupt=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mae: E[abs(score(M) - score(C))] \n",
    "- mean difference: E[score(M)] - E[score(C)] \n",
    "- frac mean difference recovered: E[score(C)] - E[score(A)] / E[score(M)] - E[score(A)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faith_metric_results_train, faith_metrics_train = compute_faith_metrics(\n",
    "    task.train_loader,\n",
    "    model_out_train,\n",
    "    ablated_out_train,\n",
    "    circuit_outs_train,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    ")\n",
    "\n",
    "faith_metric_results_test, faith_metrics_test = compute_faith_metrics(\n",
    "    task.test_loader,\n",
    "    model_out_test,\n",
    "    ablated_out_test,\n",
    "    circuit_outs_test,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    ")\n",
    "\n",
    "save_json(faith_metric_results_train, ps_dir, \"faith_metric_results_train\")\n",
    "save_json(faith_metrics_train, ps_dir, \"faith_metrics_train\")\n",
    "save_json(faith_metric_results_test, ps_dir, \"faith_metric_results_test\")\n",
    "save_json(faith_metrics_test, ps_dir, \"faith_metrics_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faith metrics eval \n",
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    faith_metric_results_train_eval, faith_metrics_train_eval = compute_faith_metrics(\n",
    "        task.train_loader,\n",
    "        model_out_train,\n",
    "        ablated_out_train,\n",
    "        circuit_outs_train,\n",
    "        conf.eval_grad_func,\n",
    "        conf.eval_answer_func,\n",
    "    )\n",
    "\n",
    "    faith_metric_results_test_eval, faith_metrics_test_eval = compute_faith_metrics(\n",
    "        task.test_loader,\n",
    "        model_out_test,\n",
    "        ablated_out_test,\n",
    "        circuit_outs_test,\n",
    "        conf.eval_grad_func,\n",
    "        conf.eval_answer_func,\n",
    "    )\n",
    "    save_json(faith_metric_results_train_eval, ps_dir, \"faith_metric_results_train_eval\")\n",
    "    save_json(faith_metrics_train_eval, ps_dir, \"faith_metrics_train_eval\")\n",
    "    save_json(faith_metric_results_test_eval, ps_dir, \"faith_metric_results_test_eval\")\n",
    "    save_json(faith_metrics_test_eval, ps_dir, \"faith_metrics_test_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalence Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_eval_metrics = conf.answer_func in DIV_ANSWER_FUNCS\n",
    "equiv_test_results_train = equiv_tests(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    grad_func=conf.grad_func if not use_eval_metrics else conf.eval_grad_func,\n",
    "    answer_func=conf.answer_func if not use_eval_metrics else conf.eval_answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    model_out=model_out_train,\n",
    "    circuit_outs=circuit_outs_train,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.epsilon,\n",
    ")\n",
    "\n",
    "equiv_test_results_test = equiv_tests(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    grad_func=conf.grad_func if not use_eval_metrics else conf.eval_grad_func,\n",
    "    answer_func=conf.answer_func if not use_eval_metrics else conf.eval_answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    model_out=model_out_test,\n",
    "    circuit_outs=circuit_outs_test,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.epsilon,\n",
    ")\n",
    "\n",
    "save_json(equiv_test_results_train, ps_dir, \"equiv_test_results_train\")\n",
    "save_json(equiv_test_results_test, ps_dir, \"equiv_test_results_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Sufficiency Test, and Expected Loss Recovered with Respect to Expected Value of Random Circuit of the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot % loss recovered and Equiv Test Results Along Frac Edges / Frac Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "from auto_circuit_tests.hypo_tests.equiv_test import EquivResult\n",
    "from auto_circuit_tests.hypo_tests.indep_test import IndepResult\n",
    "from typing import cast\n",
    "\n",
    "def plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results: Dict[int, Dict[str, float]], \n",
    "    equiv_test_results: Optional[Dict[int, Union[EquivResult, IndepResult]]]=None,\n",
    "    title: str = \"\", \n",
    "    result_type: Optional[Literal[\"equiv\", \"indep\"]] = \"equiv\",\n",
    "    x_label: str = \"Edges\"\n",
    "):\n",
    "    n_edges = list(faith_metric_results.keys())\n",
    "    fracs = [n_edge / task.model.n_edges for n_edge in n_edges]\n",
    "    frac_loss_recovered = [faith_metric_results[n_edge][\"frac_mean_diff_recovered\"] for n_edge in n_edges]\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot line\n",
    "    ax.plot(fracs, frac_loss_recovered, color='blue')\n",
    "\n",
    "    good_color = 'green'\n",
    "    bad_color = 'blue'\n",
    "\n",
    "    # color points based on hypothesis test results\n",
    "    if result_type == \"equiv\":\n",
    "        equiv_test_results = cast(Dict[int, EquivResult], equiv_test_results)\n",
    "        reject_equivs = [result.reject_equiv for result in equiv_test_results.values()]\n",
    "        reject_non_equivs = [result.reject_non_equiv for result in equiv_test_results.values()]\n",
    "        for frac, loss, reject_equiv, reject_non_equiv in zip(fracs, frac_loss_recovered, reject_equivs,reject_non_equivs):\n",
    "            color = 'light' + good_color if reject_non_equiv else (good_color if not reject_equiv else bad_color)\n",
    "            ax.scatter(frac, loss, color=color, s=100, zorder=5)  # s is the size of the dot, zorder ensures it's on top\n",
    "        \n",
    "        ax.scatter([], [], color='light' + good_color, label='Non-Equiv Rejected', s=100)\n",
    "        ax.scatter([], [], color=good_color, label='Equiv Not Rejected', s=100)\n",
    "        ax.scatter([], [], color=bad_color, label='Equiv Rejected', s=100)\n",
    "        ax.legend()\n",
    "    elif result_type == \"indep\": \n",
    "        indep_test_results = cast(Dict[int, IndepResult], equiv_test_results)\n",
    "        reject_indeps = [result.reject_null for result in indep_test_results.values()]\n",
    "        for frac, loss, reject_indep in zip(fracs, frac_loss_recovered, reject_indeps):\n",
    "            color = 'light' + good_color if not reject_indep else bad_color\n",
    "            ax.scatter(frac, loss, color=color, s=100, zorder=5)\n",
    "        \n",
    "        ax.scatter([], [], color=good_color, label='Indep Not Rejected', s=100)\n",
    "        ax.scatter([], [], color=bad_color, label='Indep Rejected', s=100)\n",
    "        \n",
    "\n",
    "    ax.set_xlabel(f\"Fraction of Total {x_label}\")\n",
    "    ax.set_ylabel(\"Fraction of Loss Recovered\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # horizontal line at 0.95\n",
    "    ax.axhline(0.95, color='r', linestyle='--')\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    x_ticks = np.arange(0, 1.0, 0.1)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels([f'{x:.1f}' for x in x_ticks])\n",
    "\n",
    "    # Set x-axis limits\n",
    "    ax.set_xlim(0, 1.0)\n",
    "\n",
    "\n",
    "    # Adjust layout to prevent clipping of labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results using div answer function (no equivalence results)\n",
    "if use_eval_metrics:\n",
    "    # TODO: figure out why this plotting is off\n",
    "    fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "        faith_metric_results_train,\n",
    "        title=\"(Train) Fraction of Loss Recovered\",\n",
    "        result_type=None,\n",
    "        x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    "    )\n",
    "    fig.savefig(edge_dir / \"frac_loss_recovered_results_train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results using div answer function (no equivalence results)\n",
    "if use_eval_metrics:\n",
    "    fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "        faith_metric_results_test,\n",
    "        title=\"(Test) Fraction of Loss Recovered\",\n",
    "        result_type=None,\n",
    "        x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    "    )\n",
    "    fig.savefig(edge_dir / \"frac_loss_recovered_results_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out why this plotting is off\n",
    "\n",
    "fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results_train if not use_eval_metrics else faith_metric_results_train_eval, \n",
    "    equiv_test_results_train,\n",
    "    title=\"(Train) Fraction of Loss Recovered and Equiv Test Results\",\n",
    "    result_type=\"equiv\",\n",
    "    x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    ")\n",
    "fig.savefig(edge_dir / \"frac_loss_recovered_and_equiv_test_results_train.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results_test if not use_eval_metrics else faith_metric_results_test_eval, \n",
    "    equiv_test_results_test,\n",
    "    title=\"(Test) Fraction of Loss Recovered and Equiv Test Results\",\n",
    "    result_type=\"equiv\",\n",
    "    x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    ")\n",
    "fig.savefig(edge_dir / \"frac_loss_recovered_and_equiv_test_results_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimality of Smallest Circuit with %loss recovered > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Smallest Equivalent Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_ps = flat_prune_scores_ordered(prune_scores, order=prune_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find smallest equiv circuit on training distribution\n",
    "edge_counts_equiv_idx = [\n",
    "    i for i, (k, v) in enumerate(equiv_test_results_train.items())\n",
    "    if faith_metric_results_train[k]['frac_mean_diff_recovered'] > 0.95 \n",
    "    #and v.reject_null \n",
    "]\n",
    "n_edges_min_equi_idx = min(edge_counts_equiv_idx) if edge_counts_equiv_idx else -1\n",
    "n_edges_min_equiv = circ_edges[n_edges_min_equi_idx]\n",
    "threshold = circ_thresholds[n_edges_min_equi_idx]\n",
    "\n",
    "# get edges of circuit\n",
    "edge_mask = {k: torch.abs(v) >= threshold for k, v in prune_scores.items()}\n",
    "edges = edges_from_mask(task.model.srcs, task.model.dests, edge_mask, task.token_circuit)\n",
    "save_json([(edge.seq_idx, edge.name) for edge in  edges], edge_dir, \"min_equiv_edges_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_task = TASK_TO_OUTPUT_ANSWER_FUNCS[conf.task] == (conf.grad_func, conf.answer_func) or conf.answer_func in DIV_ANSWER_FUNCS\n",
    "test_smallest = valid_task and len(edges) < 20_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Pruned Smallest Equivalent Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_smallest:\n",
    "    fig = draw_seq_graph(\n",
    "        model=task.model,\n",
    "        prune_scores=prune_scores,\n",
    "        score_threshold=threshold,\n",
    "        show_all_seq_pos=True,\n",
    "        orientation=\"h\",\n",
    "        display_ipython=False,#is_notebook(),\n",
    "        seq_labels=task.test_loader.seq_labels,\n",
    "    )\n",
    "    fig.write_image(repo_path_to_abs_path(edge_dir / \"smallest_equiv_circ_graph_train.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Unused Edges\n",
    "\n",
    "Note: Seems like there is some leakage, not exactly sure why, but I guess its fine, not using this anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_smallest:\n",
    "    # from auto_circuit_tests.edge_graph import find_unused_edges\n",
    "    def sum_prune_scores(edges: list[Edge]) -> t.Tensor:\n",
    "        return sum([\n",
    "            torch.abs(prune_scores[edge.dest.module_name][edge.patch_idx])\n",
    "            for edge in edges\n",
    "        ])\n",
    "    # find unused edges\n",
    "    used_edges, unused_edges, _circ_graph = find_unused_edges(\n",
    "        edges, conf.ablation_type, token_circuit=task.token_circuit, attn_only=task.model.cfg.attn_only\n",
    "        )\n",
    "    # get prune scores for each unused edge \n",
    "    unused_edge_prune_scores_train = {\n",
    "        edge: prune_scores[edge.dest.module_name][edge.patch_idx]\n",
    "        for edge in unused_edges\n",
    "    }\n",
    "    # save unused edges with prune scores\n",
    "    # save_json(unused_edge_prune_scores_train, edge_dir, \"unused_edges_train\")\n",
    "    print(f\"Fraction of unused edges: {len(unused_edges) / len(edges)}\")\n",
    "    save_json(len(unused_edges) / len(edges), edge_dir, \"frac_unused_edges\")\n",
    "    # save fraction of prune scores attributed to unused edges in circuit\n",
    "    total_circuit_prune_scores = sum_prune_scores(edges)\n",
    "    unused_edge_prune_scores_abs = sum_prune_scores(unused_edges)\n",
    "    save_json((unused_edge_prune_scores_abs / total_circuit_prune_scores).item(), edge_dir, \"frac_unused_edge_scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Pruned Smallest Circuit Still Equivalent and achieves >95% loss recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_smallest:\n",
    "    from auto_circuit.types import PruneScores\n",
    "    # get prune score mask\n",
    "    def edges_to_prune_score_mask(edges: list[Edge]) -> t.Tensor:\n",
    "        mask = task.model.new_prune_scores()\n",
    "        for edge in edges:\n",
    "            mask[edge.dest.module_name][edge.patch_idx] = 1\n",
    "        return mask\n",
    "\n",
    "    # compute circuit outputs for used edges \n",
    "    def run_circuit_from_mask(\n",
    "        mask: PruneScores, \n",
    "        dataloader: PromptDataLoader,\n",
    "    ) -> CircuitOutputs:\n",
    "        circuit_out: CircuitOutputs = run_circuits(\n",
    "            model=task.model, \n",
    "            dataloader=dataloader,\n",
    "            prune_scores=mask,\n",
    "            thresholds = [0.5],\n",
    "            patch_type=PatchType.TREE_PATCH, \n",
    "            ablation_type=conf.ablation_type,\n",
    "            reverse_clean_corrupt=False, \n",
    "        )\n",
    "        return circuit_out\n",
    "\n",
    "    used_edges_mask = edges_to_prune_score_mask(used_edges)\n",
    "    used_edges_out = run_circuit_from_mask(used_edges_mask, task.train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_smallest:\n",
    "# compute faithfulness metrics \n",
    "    faith_metric_results_used_edges, faith_metrics_used_edges = compute_faith_metrics(\n",
    "        task.train_loader,\n",
    "        model_out_train,\n",
    "        ablated_out_train,\n",
    "        used_edges_out,\n",
    "        conf.grad_func,\n",
    "        conf.answer_func,\n",
    "    )\n",
    "    print(f\"Used Edges Train %loss recovered: {list(faith_metric_results_used_edges.values())[0]['frac_mean_diff_recovered']}\")\n",
    "    save_json(faith_metric_results_used_edges, ps_dir, \"faith_metric_results_used_edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run equiv tests on used edges\n",
    "if test_smallest:\n",
    "    equiv_test_results_used_edges = equiv_tests(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader,\n",
    "        prune_scores=used_edges_mask,\n",
    "        grad_func=conf.grad_func,\n",
    "        answer_func=conf.answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        model_out=model_out_train,\n",
    "        circuit_outs=used_edges_out,\n",
    "        alpha=conf.alpha,\n",
    "        epsilon=conf.epsilon,\n",
    "    )\n",
    "    print(f\"Used Edges Null Rejected: {list(equiv_test_results_used_edges.values())[0].reject_non_equiv}\")\n",
    "    save_json(equiv_test_results_used_edges, ps_dir, \"equiv_test_results_used_edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimality Test and Change in %loss Recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run on docstring to save time\n",
    "run_min_test = test_smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Circuits with Each Edge Ablated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    edge_outs_train= run_circuit_with_edges_ablated(\n",
    "        model=task.model,\n",
    "        dataloader=task.train_loader,\n",
    "        prune_scores=prune_scores,\n",
    "        edges=edges,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    edge_outs_test = run_circuit_with_edges_ablated(\n",
    "        model=task.model,\n",
    "        dataloader=task.test_loader,\n",
    "        prune_scores=prune_scores,\n",
    "        edges=edges,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        threshold=threshold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Change in %loss recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    edge_faith_metric_results_train, edge_faith_metrics_train = compute_faith_metrics(\n",
    "        task.train_loader,\n",
    "        model_out_train,\n",
    "        ablated_out_train,\n",
    "        edge_outs_train, # NOTE - wrong data type, keys should be ints, but doesn't matter\n",
    "        conf.grad_func,\n",
    "        conf.answer_func,\n",
    "    )\n",
    "    # hmm this should just be by edge, also I want the edge order\n",
    "    save_json({edge_name(k): v for k, v in edge_faith_metric_results_train.items()}, ps_dir, \"edge_faith_metric_results_train\")\n",
    "    save_json({edge_name(k): v for k, v in edge_faith_metrics_train.items()}, ps_dir, \"edge_faith_metrics_train\")\n",
    "\n",
    "    edge_faith_metric_results_test, edge_faith_metrics_test = compute_faith_metrics(\n",
    "        task.test_loader,\n",
    "        model_out_test,\n",
    "        ablated_out_test,\n",
    "        edge_outs_test, # NOTE - wrong data type, keys should be ints, but doesn't matter\n",
    "        conf.grad_func,\n",
    "        conf.answer_func,\n",
    "    )\n",
    "    save_json({edge_name(k): v for k, v in edge_faith_metric_results_test.items()}, ps_dir, \"edge_faith_metric_results_test\")\n",
    "    save_json({edge_name(k): v for k, v in edge_faith_metrics_test.items()}, ps_dir, \"edge_faith_metrics_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # plot change in loss recovered \n",
    "    frac_loss_recovered_train = faith_metric_results_train[n_edges_min_equiv]['frac_mean_diff_recovered']\n",
    "    frac_loss_recovered_test = faith_metric_results_test[n_edges_min_equiv]['frac_mean_diff_recovered']\n",
    "    # sort edges by prune scores \n",
    "    edge_prune_scores = {\n",
    "        edge: prune_scores[edge.dest.module_name][edge.patch_idx].cpu().item()\n",
    "        for edge in edges\n",
    "    }\n",
    "    sorted_edge_prune_scores = sorted(edge_prune_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    frac_loss_recovered_train_sorted = [edge_faith_metric_results_train[edge]['frac_mean_diff_recovered'] for edge, _ in sorted_edge_prune_scores]\n",
    "    frac_loss_recovered_test_sorted = [edge_faith_metric_results_test[edge]['frac_mean_diff_recovered'] for edge, _ in sorted_edge_prune_scores]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # add transparency to lines\n",
    "    ax.plot([frac_loss_recovered_train - x for x in reversed(frac_loss_recovered_train_sorted)], label=\"Train\")\n",
    "    ax.plot([frac_loss_recovered_test - x for x in reversed(frac_loss_recovered_test_sorted)], label=\"Test\", alpha=0.75)\n",
    "    # horizontal line at 1/circuit_size\n",
    "    ax.axhline((1/ len(edges)) * 100, color='r', linestyle='--')\n",
    "\n",
    "    ax.set_xlabel(\"Edge Index\")\n",
    "    ax.set_ylabel(\"Change in Fraction of Loss Recovered\")\n",
    "    ax.set_title(\"Change in Fraction of Loss Recovered for Each Edge\")\n",
    "\n",
    "    # add legend\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Activation Patching Scores on Circuit and Correlation Between Activation Patching Scores on Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    from auto_circuit.prune_algos.utils import compute_loss \n",
    "    def compute_full_model_score(\n",
    "        model: PatchableModel,\n",
    "        dataloader: PromptDataLoader,\n",
    "        model_outs: BatchOutputs,\n",
    "        grad_func: str,\n",
    "        answer_func: str,\n",
    "    ):\n",
    "        full_model_score = 0\n",
    "        for batch in dataloader:\n",
    "            full_model_score -= compute_loss(\n",
    "                model, \n",
    "                batch, \n",
    "                grad_func, \n",
    "                answer_func, \n",
    "                logits=model_outs[batch.key].to(task.device)\n",
    "            ).sum().item()\n",
    "        return full_model_score\n",
    "\n",
    "    full_model_score = 0\n",
    "    if conf.answer_func not in DIV_ANSWER_FUNCS:\n",
    "        full_model_score = compute_full_model_score(\n",
    "            task.model,\n",
    "            task.train_loader, \n",
    "            model_out_train, \n",
    "            conf.grad_func.value, \n",
    "            conf.answer_func.value\n",
    "        )\n",
    "\n",
    "    def compute_edge_scores(\n",
    "        model: PatchableModel,\n",
    "        dataloader: PromptDataLoader,\n",
    "        edge_outs: dict[Edge, BatchOutputs],\n",
    "        model_outs: BatchOutputs,\n",
    "        grad_func: str,\n",
    "        answer_func: str,\n",
    "        full_model_score: float,\n",
    "    ) -> PruneScores:\n",
    "        edge_prune_scores = model.new_prune_scores()\n",
    "        for mod_name in edge_prune_scores.keys():\n",
    "            edge_prune_scores[mod_name] += full_model_score\n",
    "        for edge, edge_out in edge_outs.items():\n",
    "            for batch in dataloader:\n",
    "                edge_score = -compute_loss(\n",
    "                    model, \n",
    "                    batch, \n",
    "                    grad_func, \n",
    "                    answer_func, \n",
    "                    logits=edge_out[batch.key].to(task.device),\n",
    "                    clean_out=model_outs[batch.key].to(task.device)\n",
    "                ).sum().item()\n",
    "                edge_prune_scores[edge.dest.module_name][edge.patch_idx] -= edge_score\n",
    "        return edge_prune_scores\n",
    "\n",
    "\n",
    "    # edge outs train \n",
    "    edge_circ_act_prune_scores = compute_edge_scores(\n",
    "        task.model,\n",
    "        task.train_loader,\n",
    "        edge_outs_train,\n",
    "        model_out_train,\n",
    "        conf.grad_func.value,\n",
    "        conf.answer_func.value,\n",
    "        full_model_score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation\n",
    "# TODO: compute actual activation patching scores for circuit, compute actual correlation\n",
    "if run_min_test:\n",
    "    # plot correlation between minimality score (change in score) and prune score \n",
    "    edge_act_prune_scores_flat = [act_prune_scores[edge.dest.module_name][edge.patch_idx].item() for edge in edges]\n",
    "    edge_circ_act_prune_scores_flat = [edge_circ_act_prune_scores[edge.dest.module_name][edge.patch_idx].item() for edge in edges]\n",
    "    \n",
    "    # compute correlation coefficient \n",
    "    from scipy import stats\n",
    "    corr, p_value = stats.pearsonr(edge_act_prune_scores_flat, edge_circ_act_prune_scores_flat)\n",
    "    \n",
    "    plt.scatter(edge_act_prune_scores_flat, edge_circ_act_prune_scores_flat, s=1.0)\n",
    "    plt.xlabel(\"Act Patch Scores\")\n",
    "    plt.ylabel(\"Circuit Act Patch Scores\")\n",
    "    plt.title(f\"Correlation: {corr:.2f}, p-value: {p_value:.2f}\")\n",
    "    plt.savefig(edge_dir / \"circ_act_patch_corr.png\")\n",
    "\n",
    "    save_json({\"corr\": corr, \"p_value\": p_value}, edge_dir, \"circ_act_patch_corr_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # build full grap to sample paths\n",
    "    graph = SeqGraph(task.model.edges, token=task.token_circuit, attn_only=task.model.cfg.attn_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # ok so there should be columns for each sequence position, and subcolumsn for each component\n",
    "    seq_idxs = [0, task.test_loader.seq_len-1] if task.token_circuit else None\n",
    "    visualize_graph(graph, sort_by_head=False, max_layer=None, seq_idxs=seq_idxs, column_width=5, figsize=(36, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot circuit graph\n",
    "if run_min_test:\n",
    "    circ_graph = SeqGraph(edges, token=task.token_circuit, attn_only=task.model.cfg.attn_only)\n",
    "    seq_idxs = set([seq_node.seq_idx for seq_node in circ_graph.seq_nodes])\n",
    "    visualize_graph(circ_graph, sort_by_head=False, max_layer=None, seq_idxs=seq_idxs, column_width=10, figsize=(72, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample paths from complement for each data instance\n",
    "if run_min_test:\n",
    "    complement_edges = set(task.model.edges) - set(edges)\n",
    "    sampled_paths = sample_paths(\n",
    "        seq_graph=graph, \n",
    "        n_paths=conf.n_paths, \n",
    "        complement_edges=complement_edges,\n",
    "    )\n",
    "    edges_set = set(edges)\n",
    "    novel_edge_paths = [[edge for edge in path if edge not in edges_set] for path in sampled_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    path_idx = 0\n",
    "    sampled_path = sampled_paths[path_idx]\n",
    "    novel_edges = novel_edge_paths[path_idx]\n",
    "    redundant_edges = set(sampled_path).intersection(set(edges))\n",
    "    print(f\"Added edges: {novel_edges}\")\n",
    "    print(f\"Redundant edges: {redundant_edges}\")\n",
    "    ex_inflated_graph = SeqGraph(edges + list(novel_edges), token=task.token_circuit, attn_only=task.model.cfg.attn_only)\n",
    "    seq_idxs = set([seq_node.seq_idx for seq_node in ex_inflated_graph.seq_nodes]) if task.token_circuit else None\n",
    "    edge_colors = {}\n",
    "    [edge_colors.update({edge: 'blue'}) for edge in novel_edges]\n",
    "    [edge_colors.update({edge: 'darkblue'}) for edge in redundant_edges]\n",
    "    visualize_graph(ex_inflated_graph, sort_by_head=False, max_layer=None, seq_idxs=seq_idxs, edge_colors=edge_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample paths to remove \n",
    "if run_min_test:\n",
    "    ablated_paths, removed_edges = [], []\n",
    "    for path in novel_edge_paths:\n",
    "        edge_idx_to_remove = random.choice(range(len(path)))\n",
    "        ablated_path = remove_el(path, edge_idx_to_remove)\n",
    "        ablated_paths.append(ablated_path)\n",
    "        removed_edges.append(path[edge_idx_to_remove])\n",
    "    removed_edge = removed_edges[path_idx]\n",
    "    edge_colors[removed_edge] = 'red'\n",
    "    visualize_graph(ex_inflated_graph, sort_by_head=False, max_layer=None, seq_idxs=seq_idxs, edge_colors=edge_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    inflated_outs, ablated_outs = run_circuits_inflated_ablated(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        edges=edges,\n",
    "        n_paths=conf.n_paths,\n",
    "        graph=graph,\n",
    "        paths=sampled_paths,\n",
    "        ablated_paths=ablated_paths,\n",
    "        token=task.token_circuit,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # compute mean diffs for each inflated circuit / ablated circuit\n",
    "    inflated_ablated_mean_diffs: list[float] = []\n",
    "    for i, inflated_out in inflated_outs.items():\n",
    "        inflated_ablated_diffs = score_diffs(\n",
    "            dataloader=task.train_loader,\n",
    "            outs_1=inflated_out,\n",
    "            outs_2=ablated_outs[i],\n",
    "            grad_func=conf.grad_func,\n",
    "            answer_func=conf.answer_func,\n",
    "            device=task.device\n",
    "        )\n",
    "        inflated_ablated_mean_diffs.append(t.cat(inflated_ablated_diffs).mean().item())\n",
    "\n",
    "    # compute mean diffs for each ablated edge\n",
    "    ablated_edge_mean_diffs: dict[Edge, float] = {}\n",
    "    for edge in edges:\n",
    "        ablated_diffs = score_diffs(\n",
    "            dataloader=task.train_loader,\n",
    "            outs_1=edge_outs_train[edge],\n",
    "            outs_2=circuit_outs_train[n_edges_min_equiv],\n",
    "            grad_func=conf.grad_func,\n",
    "            answer_func=conf.answer_func,\n",
    "            model_outs=model_out_train,\n",
    "            device=task.device\n",
    "        )\n",
    "        ablated_edge_mean_diffs[edge] = t.cat(ablated_diffs).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit_tests.hypo_tests.minimality_test import MinResult\n",
    "if run_min_test:\n",
    "    min_results_train: dict[Edge, MinResult] = {}\n",
    "    for edge in tqdm(edges):\n",
    "        min_results_train[edge] = minimality_test_edge(\n",
    "            ablated_edge_mean_diff=ablated_edge_mean_diffs[edge],\n",
    "            inflated_ablated_mean_diffs=inflated_ablated_mean_diffs,\n",
    "            n_edges=len(edges),\n",
    "            alpha=conf.alpha, # bonferroni handled internally\n",
    "            q_star=conf.q_star,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # plot minimality scores and fraction of loss recovered sorted by minimality score with threshold for rejection (from paper)\n",
    "    edges_by_min_score = sorted(edges, key=lambda edge: ablated_edge_mean_diffs[edge], reverse=False)\n",
    "    min_scores = [ablated_edge_mean_diffs[edge] for edge in edges_by_min_score]\n",
    "    frac_loss_recovered_train = faith_metric_results_train[n_edges_min_equiv]['frac_mean_diff_recovered']\n",
    "    frac_loss_recovered_delta = [frac_loss_recovered_train - edge_faith_metric_results_train[edge]['frac_mean_diff_recovered'] for edge in edges_by_min_score]\n",
    "    \n",
    "    # get first edge which does not reject miniamlity, and first edge that rejects non-minimality\n",
    "    first_min_not_rejected = [min_results_train[edge].reject_min_null for edge in edges_by_min_score].index(False)\n",
    "    first_not_min_rejected = [min_results_train[edge].reject_null_non_min for edge in edges_by_min_score].index(True)\n",
    "\n",
    "    # plot minimality scores and fraction of loss recovered\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(min_scores, label=\"Change in Score\")\n",
    "    ax.set_xlabel(\"Edge Index\")\n",
    "    ax.set_ylabel(\"Change in Score\")\n",
    "    # ax.set_yscale('log')\n",
    "    # new axis for fraction of loss recovered\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(frac_loss_recovered_delta, label=\"Change in Fraction of Loss Recovered\", color='orange', alpha=0.75)\n",
    "    ax2.set_ylabel(\"Change in Fraction of Loss Recovered\")\n",
    "\n",
    "    # add vertical line and shaed region for first edge that does not reject minimality\n",
    "    ax.axvline(first_min_not_rejected, color='blue', linestyle='--')\n",
    "    ax.axvspan(0,first_min_not_rejected, color='lightblue', alpha=0.5)\n",
    "    # add veritical line and shaded region for first edge that rejects non-minimality\n",
    "    ax.axvline(first_not_min_rejected, color='green', linestyle='--')\n",
    "    ax.axvspan(first_min_not_rejected, first_not_min_rejected, color='lightgreen', alpha=0.5)\n",
    "    # TODO: put fig legend where ax legend would be \n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.15, 0.95))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    min_results_test, null_rejected_test = minimality_test(\n",
    "        model=task.model, \n",
    "        dataloader=task.test_loader,\n",
    "        edges=edges,\n",
    "        prune_scores=prune_scores,\n",
    "        threshold=threshold,\n",
    "        grad_func=conf.grad_func,\n",
    "        answer_func=conf.answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        token=task.token_circuit,\n",
    "        model_outs=model_out_test,\n",
    "        n_paths=conf.n_paths,\n",
    "        q_star=conf.q_star,\n",
    "        device=task.device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimality Test on \"Ground Truth\" Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK_TO_OUTPUT_ANSWER_FUNCS[task.key] == (conf.grad_func, conf.answer_func):\n",
    "    # inflated ablated \n",
    "    inflated_outs_true, ablated_outs_true = run_circuits_inflated_ablated(\n",
    "        model=task.model,\n",
    "        dataloader=task.test_loader,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        edges=edges,\n",
    "        n_paths=conf.n_paths,\n",
    "        token=task.token_circuit\n",
    "    )\n",
    "\n",
    "    # compute mean diffs for each inflated circuit / ablated circuit\n",
    "    inflated_ablated_mean_diffs_true: list[float] = []\n",
    "    for i, inflated_out in inflated_outs_true.items():\n",
    "        inflated_ablated_diffs = score_diffs(\n",
    "            dataloader=task.test_loader,\n",
    "            outs_1=inflated_out,\n",
    "            outs_2=ablated_outs_true[i],\n",
    "            grad_func=conf.grad_func,\n",
    "            answer_func=conf.answer_func,\n",
    "            model_outs=model_out_test,\n",
    "            device=task.device\n",
    "        )\n",
    "        inflated_ablated_mean_diffs_true.append(t.cat(inflated_ablated_diffs).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK_TO_OUTPUT_ANSWER_FUNCS[task.key] == (conf.grad_func, conf.answer_func):\n",
    "    true_edges_min_test_results, null_rejected = minimality_test(\n",
    "        model=task.model, \n",
    "        dataloader=task.test_loader,\n",
    "        edges=list(task.true_edges),\n",
    "        prune_scores=task.model.circuit_prune_scores(task.true_edges),\n",
    "        threshold=0.5,\n",
    "        grad_func=conf.grad_func,\n",
    "        answer_func=conf.answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        token=task.token_circuit,\n",
    "        inflated_outs=inflated_outs_true,\n",
    "        ablated_outs=ablated_outs_true,\n",
    "        q_star=conf.q_star,\n",
    "        device=task.device,\n",
    "        stop_if_reject=True\n",
    "    )\n",
    "    save_json({edge_name(k): v for k, v in true_edges_min_test_results.items()}, edge_dir, \"true_edges_min_test_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence Test and Complement %Loss Recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % Loss Recovered of Complement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complement outs\n",
    "complement_outs_train = run_circuits(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    test_edge_counts=circ_edges,\n",
    "    patch_type=PatchType.EDGE_PATCH, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    reverse_clean_corrupt=True, # ablated edges are corrupt \n",
    ")\n",
    "\n",
    "complement_outs_test: CircuitOutputs = run_circuits(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    test_edge_counts=circ_edges,\n",
    "    patch_type=PatchType.EDGE_PATCH, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    reverse_clean_corrupt=True, # ablated edges are corrupt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get faithfulness metrics of complement\n",
    "faith_metric_results_c_train, faith_metrics_c_train = compute_faith_metrics(\n",
    "    task.train_loader,\n",
    "    model_out_train, \n",
    "    ablated_out_train,\n",
    "    complement_outs_train,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    ")\n",
    "\n",
    "\n",
    "faith_metric_results_c_test, faith_metrics_c_test = compute_faith_metrics(\n",
    "    task.test_loader,\n",
    "    model_out_test,\n",
    "    ablated_out_test,\n",
    "    complement_outs_test,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    ")\n",
    "\n",
    "\n",
    "save_json(faith_metric_results_c_train, ps_dir, \"faith_metric_results_c_train\")\n",
    "save_json(faith_metrics_c_train, ps_dir, \"faith_metrics_c_train\")\n",
    "save_json(faith_metric_results_c_test, ps_dir, \"faith_metric_results_c_test\")\n",
    "save_json(faith_metrics_c_test, ps_dir, \"faith_metrics_c_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute faithfulness metrics using eval functions\n",
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    faith_metric_results_c_train_eval, faith_metrics_c_train_eval = compute_faith_metrics(\n",
    "        task.train_loader,\n",
    "        model_out_train,\n",
    "        ablated_out_train,\n",
    "        complement_outs_train,\n",
    "        conf.eval_grad_func,\n",
    "        conf.eval_answer_func,\n",
    "    )\n",
    "\n",
    "    faith_metric_results_c_test_eval, faith_metrics_c_test_eval = compute_faith_metrics(\n",
    "        task.test_loader,\n",
    "        model_out_test,\n",
    "        ablated_out_test,\n",
    "        complement_outs_test,\n",
    "        conf.eval_grad_func,\n",
    "        conf.eval_answer_func,\n",
    "    )\n",
    "    save_json(faith_metric_results_c_train_eval, ps_dir, \"faith_metric_results_c_train_eval\")\n",
    "    save_json(faith_metrics_c_train_eval, ps_dir, \"faith_metrics_c_train_eval\")\n",
    "    save_json(faith_metric_results_c_test_eval, ps_dir, \"faith_metric_results_c_test_eval\")\n",
    "    save_json(faith_metrics_c_test_eval, ps_dir, \"faith_metrics_c_test_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence HCIC (Frequentist) Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Test for completeness - if the circuit contains all the components required to perform the task, then the output of the complement should be independent of the original model\n",
    "\n",
    "$H_0$: Score of complement indepedendent of score of model\n",
    "\n",
    "Hilbert Schmdit Indepednence Criterion - non-parametric measure of independence \n",
    "\n",
    "- Background: (see https://jejjohnson.github.io/research_journal/appendix/similarity/hsic/)\n",
    "\n",
    "Intuition: the trace sums along the interaction terms on each data point, which \n",
    "we expect to be larger then other interaction terms across samples if X, and Y are \n",
    "correlated, fewer of the perumations should be greater, our p-value will be smaller, \n",
    "and thus we're more likely to reject the null\n",
    "\n",
    "\n",
    "Note: the hypothesis paper defines HCIC as  K_{x,y}K_{x,y}, but can also define it as \n",
    "{K_x}{K_y}, b/c that that equality holds in general for Cross Covariance and Auto \n",
    "Covariance \n",
    "\n",
    "The paper uses $\\rho$ = median(||score(complement) - score(model)||), based on this \n",
    "paper https://arxiv.org/pdf/1707.07269\n",
    "\n",
    "I'm not sure if we can do an interval test, because it seems like we need to assume \n",
    "a kind of uniform null - I basically don't understand the test enough\n",
    "\n",
    "I want to say something like independent only if \"p value\" between 0.5 +- epsilon \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit_tests.hypo_tests.indep_test import independence_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_results_train = independence_tests(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader, \n",
    "    prune_scores=prune_scores, \n",
    "    grad_func=conf.grad_func,\n",
    "    answer_func=conf.answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    model_out=model_out_train,\n",
    "    complement_circuit_outs=complement_outs_train,\n",
    "    alpha=conf.alpha,\n",
    "    B=1000\n",
    ")\n",
    "save_json(indep_results_train, ps_dir, \"indep_results_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_results_test = independence_tests(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader, \n",
    "    prune_scores=prune_scores, \n",
    "    grad_func=conf.grad_func,\n",
    "    answer_func=conf.answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    model_out=model_out_test,\n",
    "    complement_circuit_outs=complement_outs_test,\n",
    "    alpha=conf.alpha,\n",
    "    B=1000\n",
    ")\n",
    "\n",
    "save_json(indep_results_test, ps_dir, \"indep_results_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    indep_results_train_eval = independence_tests(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader, \n",
    "        prune_scores=prune_scores, \n",
    "        grad_func=conf.eval_grad_func,\n",
    "        answer_func=conf.eval_answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        model_out=model_out_train,\n",
    "        complement_circuit_outs=complement_outs_train,\n",
    "        alpha=conf.alpha,\n",
    "        B=1000\n",
    "    )\n",
    "    save_json(indep_results_train_eval, ps_dir, \"indep_results_train_eval\")\n",
    "\n",
    "    indep_results_test_eval = independence_tests(\n",
    "        model=task.model, \n",
    "        dataloader=task.test_loader, \n",
    "        prune_scores=prune_scores, \n",
    "        grad_func=conf.eval_grad_func,\n",
    "        answer_func=conf.eval_answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        model_out=model_out_test,\n",
    "        complement_circuit_outs=complement_outs_test,\n",
    "        alpha=conf.alpha,\n",
    "        B=1000\n",
    "    )\n",
    "    save_json(indep_results_test_eval, ps_dir, \"indep_results_test_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot % loss recovered and indep test results\n",
    "fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results_c_train, \n",
    "    indep_results_train,\n",
    "    title=\"(Train) Fraction of Loss Recovered by Complement and Independence Test Results\",\n",
    "    null_good=True,\n",
    "    x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    ")\n",
    "fig.savefig(edge_dir / \"frac_loss_recovered_and_indep_test_results_train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot % loss recovered and indep test results\n",
    "fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results_c_test, \n",
    "    indep_results_test,\n",
    "    title=\"(Test) Fraction of Loss Recovered by Complement and Independence Test Results\",\n",
    "    \n",
    "    x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    ")\n",
    "fig.savefig(edge_dir / \"frac_loss_recovered_and_indep_test_results_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "        faith_metric_results_c_train_eval, \n",
    "        indep_results_train_eval,\n",
    "        title=\"(Train) Fraction of Loss Recovered by Complement and Independence Test Results\",\n",
    "        null_good=True,\n",
    "        x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    "    )\n",
    "    fig.savefig(edge_dir / \"frac_loss_recovered_and_indep_test_results_train_eval.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "        faith_metric_results_c_test_eval, \n",
    "        indep_results_test_eval,\n",
    "        title=\"(Test) Fraction of Loss Recovered by Complement and Independence Test Results\",\n",
    "        null_good=True,\n",
    "        x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    "    )\n",
    "    fig.savefig(edge_dir / \"frac_loss_recovered_and_indep_test_results_test_eval.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Independence Test on True Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK_TO_OUTPUT_ANSWER_FUNCS[task.key] == (conf.grad_func, conf.answer_func):\n",
    "    indep_true_edge_result_test = next(iter(independence_tests(\n",
    "        task.model, \n",
    "        task.test_loader, \n",
    "        task.model.circuit_prune_scores(task.true_edges), \n",
    "        ablation_type=conf.ablation_type,\n",
    "        grad_func=conf.grad_func,\n",
    "        answer_func=conf.answer_func,\n",
    "        thresholds=[0.5], \n",
    "        model_out=model_out_test,\n",
    "        alpha=conf.alpha,\n",
    "        B=1000\n",
    "    ).values()))\n",
    "    save_json(result_to_json(indep_true_edge_result_test), out_answer_dir, f\"indep_true_edge_result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
