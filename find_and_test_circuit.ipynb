{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" #\"1\"\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    # os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing Automatically Discovered Circuits\n",
    "\n",
    "Procedure: \n",
    "- Compute prune scores (via attribution patching) \n",
    "- Search over different thresholds to find the smallest circuit where the null hypotheis of Equivalence / Dominance cannot be rejected \n",
    "- Prune edges from circuit that are not in paths to the output, or in the case of resample ablation cannot be reached from the input\n",
    "- Test whether each edge in the circuit is minimal \n",
    "- Test whether the circuit is complete (by seeing if the null hypothesis on the independence test can be rejected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Callable, Dict, Tuple, Union, Optional, Any, Literal, NamedTuple\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from scipy.stats import binom, beta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "from auto_circuit.data import PromptPairBatch, PromptDataLoader\n",
    "from auto_circuit.utils.patchable_model import PatchableModel\n",
    "from auto_circuit.types import (BatchKey, PruneScores, CircuitOutputs, AblationType, \n",
    "                                Edge, BatchOutputs, EdgeCounts)\n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.prune_algos.activation_patching import act_patch_prune_scores\n",
    "from auto_circuit.prune_algos.ACDC import acdc_prune_scores\n",
    "from auto_circuit.prune_algos.subnetwork_probing import subnetwork_probing_prune_scores\n",
    "from auto_circuit.prune_algos.circuit_probing import circuit_probing_prune_scores\n",
    "from auto_circuit.visualize import draw_seq_graph\n",
    "from auto_circuit.utils.custom_tqdm import tqdm\n",
    "from auto_circuit.utils.tensor_ops import desc_prune_scores\n",
    "from auto_circuit.utils.graph_utils import edge_counts_util\n",
    "\n",
    "from auto_circuit_tests.prune_algos import PruneAlgo\n",
    "from auto_circuit_tests.score_funcs import GradFunc, AnswerFunc, DIV_ANSWER_FUNCS\n",
    "# from auto_circuit_tests.faithful_metrics import FaithfulMetric\n",
    "\n",
    "from auto_circuit_tests.edge_scores import compute_edge_scores\n",
    "from auto_circuit_tests.utils.auto_circuit_utils import (\n",
    "    run_circuit_with_edge_ablated,\n",
    "    run_circuit_with_edges_ablated, \n",
    "    run_fully_ablated_model, \n",
    "    flat_prune_scores_ordered\n",
    ")\n",
    "\n",
    "from auto_circuit_tests.faith_metrics import compute_faith_metrics\n",
    "from auto_circuit_tests.hypo_tests.equiv_test import equiv_tests\n",
    "from auto_circuit_tests.hypo_tests.minimality_test import (\n",
    "    run_circuits_inflated_ablated, \n",
    "    score_diffs,\n",
    "    minimality_test_edge,\n",
    "    minimality_test, \n",
    ")\n",
    "from auto_circuit_tests.hypo_tests.indep_test import independence_tests, indep_test\n",
    "from auto_circuit_tests.hypo_tests.utils import (\n",
    "    join_values, \n",
    "    remove_el,\n",
    "    edges_from_mask, \n",
    "    result_to_json, \n",
    ")\n",
    "from auto_circuit_tests.edge_graph import (\n",
    "    SeqGraph,  \n",
    "    sample_paths, \n",
    "    SampleType,\n",
    "    edge_in_path, \n",
    "    find_unused_edges,\n",
    "    visualize_graph\n",
    ")\n",
    "\n",
    "from auto_circuit_tests.tasks import TASK_DICT, TASK_TO_OUTPUT_ANSWER_FUNCS\n",
    "from auto_circuit_tests.utils.auto_circuit_utils import edge_name\n",
    "from auto_circuit_tests.utils.utils import (\n",
    "    repo_path_to_abs_path, \n",
    "    load_cache, \n",
    "    save_cache, \n",
    "    save_json, \n",
    "    load_json, # should probably move this to auto_circuit_tests.utils\n",
    "    get_el_rank\n",
    ")\n",
    "from auto_circuit_tests.utils.utils import get_exp_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class\n",
    "from dataclasses import dataclass, field\n",
    "@dataclass \n",
    "class Config: \n",
    "    task: str = \"Indirect Object Identification Token Circuit\" # check how many edges in component circuit (probably do all but ioi toen)\n",
    "    ablation_type: AblationType = AblationType.RESAMPLE\n",
    "    grad_func: GradFunc = GradFunc.LOGPROB\n",
    "    answer_func: AnswerFunc = AnswerFunc.KL_DIV\n",
    "    eval_grad_func: Optional[GradFunc] = None # TODO: used to evaluate faithfulness\n",
    "    prune_algo: PruneAlgo = PruneAlgo.ATTR_PATCH\n",
    "    eval_answer_func: Optional[AnswerFunc] = None\n",
    "    ig_samples: Optional[int] = 50\n",
    "    layerwise: bool = False\n",
    "    edge_counts: EdgeCounts = EdgeCounts.LOGARITHMIC\n",
    "    tao_bases: list[float] = field(default_factory=lambda: [1, 5])\n",
    "    tao_exps: list[float] = field(default_factory=lambda: list(range(-5, -1)))\n",
    "    prune_score_thresh: bool = False\n",
    "    null_good: bool = True\n",
    "    alpha: float = 0.05\n",
    "    epsilon: Optional[float] = 0.1\n",
    "    q_star: float = 0.9 \n",
    "    n_paths: int = 200\n",
    "    sample_type: SampleType = SampleType.RANDOM_WALK\n",
    "    # TODO: remove these?\n",
    "    min_equiv_all_edges_thresh = 1000\n",
    "    max_edges_to_test_in_order: int = 0 #TODO: change to 125\n",
    "    max_edges_to_test_without_fail: int = 500 #TODO: change to 125\n",
    "    save_cache: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # always override clean_corrupt for now\n",
    "        self.clean_corrupt = \"corrupt\" if self.ablation_type == AblationType.RESAMPLE else None\n",
    "\n",
    "        # set eval_grad_func and faith_answer_func to logit and max diff if answer_func is div\n",
    "        if self.answer_func in DIV_ANSWER_FUNCS: \n",
    "            if self.eval_grad_func is None:\n",
    "                self.eval_grad_func = GradFunc.LOGIT\n",
    "            if self.eval_answer_func is None:\n",
    "                self.eval_answer_func = AnswerFunc.MAX_DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config \n",
    "conf = Config()\n",
    "#get config overrides if runnign from command line\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    conf_dict = OmegaConf.merge(OmegaConf.structured(conf), OmegaConf.from_cli(sys.argv[1:]))\n",
    "    conf = Config(**conf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle directories\n",
    "task_dir, ablation_dir, out_answer_dir, ps_dir, edge_dir, exp_dir = get_exp_dir(\n",
    "    task_key=conf.task, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    grad_func=conf.grad_func,\n",
    "    answer_func=conf.answer_func,\n",
    "    prune_algo=conf.prune_algo,\n",
    "    ig_samples=conf.ig_samples,\n",
    "    layerwise=conf.layerwise,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.epsilon,\n",
    "    q_star=conf.q_star,\n",
    "    prune_score_thresh=conf.prune_score_thresh,\n",
    ")\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "seq_len before divergence 15\n",
      "seq_len after divergence 13\n"
     ]
    }
   ],
   "source": [
    "# initialize task\n",
    "task = TASK_DICT[conf.task]\n",
    "# IOI model is bigger, need smaller batch size to fit on A4000\n",
    "if \"Indirect Object Identification\" in task.key:\n",
    "    task.batch_size = 32 # int(task.batch_size / 2) \n",
    "    task.batch_count = 8 # int(task.batch_count * 2)\n",
    "task.shuffle = False\n",
    "task.init_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACDC Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1575203/3983879698.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  auto_prune_scores = torch.load(auto_ps_path)\n"
     ]
    }
   ],
   "source": [
    "if conf.prune_algo == PruneAlgo.ACDC:\n",
    "    auto_ps_path = out_answer_dir / \"auto_prune_scores.pkl\"\n",
    "    if auto_ps_path.exists():\n",
    "        auto_prune_scores = torch.load(auto_ps_path)\n",
    "    else:\n",
    "        if conf.task == \"Indirect Object Identification Token Circuit\":\n",
    "            print(\"cannot run ACDC on IOI token circuit, too large\")\n",
    "            exit()\n",
    "        auto_prune_scores = acdc_prune_scores(\n",
    "            model=task.model, \n",
    "            dataloader=task.train_loader, \n",
    "            official_edges=None,\n",
    "            tao_exps=conf.tao_exps,\n",
    "            tao_bases=conf.tao_bases,\n",
    "            faithfulness_target=conf.answer_func.value, \n",
    "        )\n",
    "        if conf.save_cache:\n",
    "            torch.save(auto_prune_scores, auto_ps_path)\n",
    "    taos = sorted([tao_base * 10**tao_exp for tao_base, tao_exp in product(conf.tao_bases, conf.tao_exps)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Probing Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.CIRC_PROBE and conf.answer_func == AnswerFunc.KL_DIV:\n",
    "    circ_probe_ps_path = out_answer_dir / \"circ_probe_prune_scores.pkl\"\n",
    "    # if circ_probe_ps_path.exists():\n",
    "    #     circ_probe_prune_scores = torch.load(circ_probe_ps_path)\n",
    "    # else:\n",
    "    circ_probe_prune_scores = circuit_probing_prune_scores(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader, \n",
    "        official_edges=None,\n",
    "        tree_optimisation=True,\n",
    "        faithfulness_target=conf.answer_func.value, \n",
    "        circuit_sizes=edge_counts_util(task.model.edges, conf.edge_counts, zero_edges=False),\n",
    "        learning_rate=0.1,\n",
    "        epochs=100, \n",
    "        regularize_lambda=0.1, \n",
    "        show_train_graph=is_notebook(),\n",
    "    )\n",
    "    if conf.save_cache:\n",
    "        torch.save(circ_probe_prune_scores, circ_probe_ps_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Patching Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1575203/3009715161.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  act_prune_scores = torch.load(act_ps_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# load from cache if exists \n",
    "act_ps_path = out_answer_dir / \"act_patch_prune_scores.pt\"\n",
    "if act_ps_path.exists():\n",
    "    act_prune_scores = torch.load(act_ps_path, map_location=\"cpu\")\n",
    "else:\n",
    "    act_prune_scores = None\n",
    "\n",
    "# # if act_patch and act_patch doesn't exist, exit\n",
    "# if conf.prune_algo == PruneAlgo.ACT_PATCH and act_prune_scores is None:\n",
    "#     print(\"act_patch_prune_scores.pkl not found, exiting\")\n",
    "#     exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Attribution Patching Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer: 0          | 0/1 [01:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# if (attr_ps_path).exists():\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     attr_prune_scores = torch.load(attr_ps_path)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# else: \u001b[39;00m\n\u001b[1;32m      7\u001b[0m max_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([edge\u001b[38;5;241m.\u001b[39msrc\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m task\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39medges])\n\u001b[0;32m----> 9\u001b[0m attr_prune_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmask_gradient_prune_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mofficial_edges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#answer_func,\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mig_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mablation_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mablation_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintegrated_grad_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mig_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayerwise\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_corrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_corrupt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m attr_prune_scores \u001b[38;5;241m=\u001b[39m {mod_name: score\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m mod_name, score \u001b[38;5;129;01min\u001b[39;00m attr_prune_scores\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39msave_cache:\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/auto-circuit-tests/auto-circuit/auto_circuit/prune_algos/mask_gradient.py:110\u001b[0m, in \u001b[0;36mmask_gradient_prune_scores\u001b[0;34m(model, dataloader, official_edges, grad_function, answer_function, mask_val, integrated_grad_samples, ablation_type, clean_corrupt, layers)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    109\u001b[0m     set_all_masks(model, val\u001b[38;5;241m=\u001b[39mval)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    111\u001b[0m     patch_src_outs \u001b[38;5;241m=\u001b[39m src_outs[batch\u001b[38;5;241m.\u001b[39mkey]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch_mode(model, patch_src_outs):\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/auto-circuit-tests/auto-circuit/auto_circuit/data.py:91\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Sometimes each prompt has a different number of wrong answers\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     wrong_answers \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mwrong_answers \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m---> 91\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhash\u001b[39m((\u001b[38;5;28mstr\u001b[39m(\u001b[43mclean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28mstr\u001b[39m(corrupt\u001b[38;5;241m.\u001b[39mtolist())))\n\u001b[1;32m     93\u001b[0m diverge_idxs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m~\u001b[39m(clean \u001b[38;5;241m==\u001b[39m corrupt))\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     94\u001b[0m batch_dvrg_idx: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(diverge_idxs\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH:\n",
    "    attr_ps_name = \"attrib_patch_prune_scores\"\n",
    "    attr_ps_path = (ps_dir / attr_ps_name).with_suffix(\".pt\")\n",
    "    # if (attr_ps_path).exists():\n",
    "    #     attr_prune_scores = torch.load(attr_ps_path)\n",
    "    # else: \n",
    "    max_layer = max([edge.src.layer for edge in task.model.edges])\n",
    "\n",
    "    attr_prune_scores = mask_gradient_prune_scores(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader,\n",
    "        official_edges=None,\n",
    "        grad_function=conf.grad_func.value, \n",
    "        answer_function=conf.answer_func.value, #answer_func,\n",
    "        mask_val=0.0 if conf.ig_samples is None else None, \n",
    "        ablation_type=conf.ablation_type,\n",
    "        integrated_grad_samples=conf.ig_samples, \n",
    "        layers=max_layer if conf.layerwise else None,\n",
    "        clean_corrupt=conf.clean_corrupt,\n",
    "    )\n",
    "    attr_prune_scores = {mod_name: score.to(\"cpu\") for mod_name, score in attr_prune_scores.items()}\n",
    "    if conf.save_cache:\n",
    "        torch.save(attr_prune_scores, attr_ps_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare Activation and Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    # order = sorted(list(act_prune_scores.keys()), key=lambda x: int(x.split('.')[1]))\n",
    "    order = list(act_prune_scores.keys())\n",
    "    act_prune_scores_flat = flat_prune_scores_ordered(act_prune_scores, order=order)\n",
    "    attr_prune_scores_flat = flat_prune_scores_ordered(attr_prune_scores, order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse and median se\n",
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    mse_result_name = \"act_attr_mse\"\n",
    "    mse_result_path = (ps_dir / mse_result_name).with_suffix(\".json\")\n",
    "    if mse_result_path.exists():\n",
    "        mse_result = load_json(ps_dir, mse_result_name + '.json')\n",
    "    else:\n",
    "        prune_score_diffs = [\n",
    "            (act_prune_scores[mod_name] - attr_prune_scores[mod_name]).flatten()\n",
    "            for mod_name, _patch_mask in task.model.patch_masks.items()\n",
    "        ]\n",
    "        sq_error = torch.concat(prune_score_diffs).pow(2)\n",
    "        median_se = sq_error.median()\n",
    "        mean_se = sq_error.mean()\n",
    "        mse_result = {\n",
    "            \"median_se\": median_se.item(),\n",
    "            \"mean_se\": mean_se.item(),\n",
    "        }\n",
    "        save_json(mse_result, ps_dir, mse_result_name)\n",
    "    print(mse_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    from scipy import stats \n",
    "    abs_corr, abs_p_value = stats.spearmanr(act_prune_scores_flat.abs().cpu(), attr_prune_scores_flat.abs().cpu())\n",
    "    corr, p_value = stats.spearmanr(act_prune_scores_flat.cpu(), attr_prune_scores_flat.cpu())\n",
    "    print(f\"abs corr: {abs_corr}, abs p-value: {abs_p_value}\")\n",
    "    print(f\"corr: {corr}, p-value: {p_value}\")\n",
    "\n",
    "    spearman_results = {\n",
    "        \"abs_corr\": abs_corr,\n",
    "        \"abs_p_value\": abs_p_value,\n",
    "        \"corr\": corr,\n",
    "        \"p_value\": p_value,\n",
    "    }\n",
    "    save_json(spearman_results, ps_dir, \"spearman_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rank for scores\n",
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    act_prune_scores_rank = get_el_rank(act_prune_scores_flat.cpu())\n",
    "    attr_prune_scores_rank = get_el_rank(attr_prune_scores_flat.cpu())\n",
    "\n",
    "    act_prune_scores_0 = (act_prune_scores_flat == 0).cpu()\n",
    "    act_prune_scores_0_rank = act_prune_scores_rank[act_prune_scores_0]\n",
    "    min_0_rank, max_0_rank = act_prune_scores_0_rank.min().item(), act_prune_scores_0_rank.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    # TODO: plot x=0\n",
    "    plt.scatter(act_prune_scores_rank, attr_prune_scores_rank, s=0.1)\n",
    "    # plot min rank, max rank as vertical lines\n",
    "    plt.axvline(min_0_rank, color='blue', linestyle='--')\n",
    "    plt.axvline(max_0_rank, color='blue', linestyle='--')\n",
    "    # shade area between min and max rank\n",
    "    plt.axvspan(min_0_rank, max_0_rank, color='lightblue', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel(\"Act Patch Rank\")\n",
    "    plt.ylabel(\"Attrib Patch Rank\")\n",
    "    plt.title(\"Rank Correlation\")\n",
    "\n",
    "    plt.savefig(ps_dir / \"rank_corr.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I think there must be a bug? \n",
    "# get rank for scores\n",
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    act_prune_scores_abs_rank = get_el_rank(act_prune_scores_flat.abs().cpu())\n",
    "    attr_prune_scores_abs_rank = get_el_rank(attr_prune_scores_flat.abs().cpu())\n",
    "\n",
    "    max_0_rank = act_prune_scores_abs_rank[act_prune_scores_0].max().item()\n",
    "\n",
    "    plt.scatter(act_prune_scores_abs_rank, attr_prune_scores_abs_rank, s=0.1)\n",
    "    # plot max rank as vertical lines\n",
    "    plt.axvline(max_0_rank, color='blue', linestyle='--')\n",
    "    # shade area between min and max rank\n",
    "    plt.axvspan(0, max_0_rank, color='lightblue', alpha=0.5)\n",
    "\n",
    "    \n",
    "    plt.xlabel(\"Act Patch Rank\")\n",
    "    plt.ylabel(\"Attrib Patch Rank\")\n",
    "    plt.title(\"Rank Correlation Abs\")\n",
    "    plt.savefig(ps_dir / \"rank_corr_abs.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Fraction of \"Mis-Signed\" Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    num_missigned = (act_prune_scores_flat.sign() != attr_prune_scores_flat.sign()).sum()\n",
    "    frac_missigned = num_missigned / len(act_prune_scores_flat)\n",
    "    print(f\"Fraction of missigned: {frac_missigned}\")\n",
    "    save_json({\"frac_missigned\": frac_missigned.item()}, ps_dir, \"missigned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Fraction of Edges Recovered for Each Edge Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for different edge thresholds, compute fraction of edges not included in top k\n",
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    edge_counts = edge_counts_util(task.model.edges, conf.edge_counts, zero_edges=True)\n",
    "\n",
    "    frac_edges_recovered: Dict[int, float] = {}\n",
    "\n",
    "    for edge_count in edge_counts:\n",
    "        # get indicies where act_prune_scores_abs_rank >= task.n_edges - edge_count\n",
    "        act_indices = act_prune_scores_abs_rank >= task.model.n_edges - edge_count\n",
    "        frac_edges_recovered[edge_count] = (attr_prune_scores_abs_rank[act_indices] >= task.model.n_edges - edge_count).to(t.float).mean().item()\n",
    "\n",
    "    save_json(frac_edges_recovered, ps_dir, \"frac_edges_recovered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parition by Dest Component\n",
    "\n",
    "We partion by Dest B/c we expect difficulties to arise from estimating effects that route through non-linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit_tests.edge_graph import NodeType\n",
    "\n",
    "def mod_name_to_layer_and_node_type(mod_name: str) -> Tuple[int, NodeType]:\n",
    "    _blocks, layer, node_type_str = mod_name.split('.')\n",
    "    layer = int(layer)\n",
    "    if node_type_str == \"hook_k_input\":\n",
    "        node_type = NodeType.K \n",
    "    elif node_type_str == \"hook_q_input\":\n",
    "        node_type = NodeType.Q\n",
    "    elif node_type_str == \"hook_v_input\":\n",
    "        node_type = NodeType.V\n",
    "    elif node_type_str == \"hook_resid_post\":\n",
    "        node_type = NodeType.RESID_END \n",
    "    elif node_type_str == \"hook_mlp_in\":\n",
    "        node_type = NodeType.MLP\n",
    "    else: \n",
    "        raise ValueError(f\"Unknown node type: {node_type_str}\")\n",
    "    return layer, node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH:\n",
    "    from auto_circuit_tests.edge_graph import NodeType\n",
    "    # compute ranking by flatten by order, including module name \n",
    "    def prune_score_rankings_by_component(\n",
    "        prune_scores: PruneScores, \n",
    "        prune_scores_rank: torch.Tensor, \n",
    "        order: list[str]\n",
    "    ) -> dict[tuple[int, NodeType], list[int]]:\n",
    "        # collect mod_name ranking tuples\n",
    "        flat_mod_names = [] \n",
    "        for mod_name in order:\n",
    "            flat_mod_names.extend([mod_name for _ in range(prune_scores[mod_name].numel())])\n",
    "        # get ranking by component type and layer\n",
    "        rank_by_component: dict[tuple[int, NodeType], list[int]] = defaultdict(list)\n",
    "        for mod_name, rank in zip(flat_mod_names, prune_scores_rank):\n",
    "            layer, node_type = mod_name_to_layer_and_node_type(mod_name)\n",
    "            rank_by_component[(layer, node_type)].append(rank)\n",
    "        return rank_by_component\n",
    "\n",
    "    act_rank_by_component = prune_score_rankings_by_component(act_prune_scores, act_prune_scores_abs_rank, order)\n",
    "    attr_rank_by_component = prune_score_rankings_by_component(attr_prune_scores, attr_prune_scores_abs_rank, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # plot ranks for each component type all in one figure\n",
    "    n_layers = max([layer for layer, _ in act_rank_by_component.keys()])\n",
    "    components = sorted(list(set([node_type for _, node_type in act_rank_by_component.keys()])), key=lambda x: x.value)\n",
    "\n",
    "    # Create a 2D array to store the Axes objects\n",
    "    axs = np.empty((len(components), n_layers + 1), dtype=object)\n",
    "\n",
    "    # Create the figure without subplots initially\n",
    "    fig = plt.figure(figsize=(3 * (n_layers+1), 3 * len(components)))\n",
    "\n",
    "\n",
    "    rank_correlations: dict[tuple[int, NodeType], float] = {}\n",
    "    for layer in range(0, n_layers + 1):\n",
    "        for i, node_type in enumerate(components):\n",
    "            act_ranks = act_rank_by_component[(layer, node_type)]\n",
    "            attr_ranks = attr_rank_by_component[(layer, node_type)]\n",
    "            \n",
    "            if len(act_ranks) == 0 and len(attr_ranks) == 0:\n",
    "                continue\n",
    "\n",
    "            # compute rank correlation\n",
    "            corr, p_value = stats.spearmanr(act_ranks, attr_ranks)\n",
    "            rank_correlations[(layer, node_type)] = corr\n",
    "            \n",
    "            # Create a subplot only if there's data to plot\n",
    "            ax = fig.add_subplot(len(components), (n_layers+1), (i * (n_layers+1)) + layer+1)\n",
    "            ax.scatter(act_ranks, attr_ranks, s=1)\n",
    "            # set title below scatter plot\n",
    "            ax.set_title(f\"Correlation: {corr:.2f}\", y=-0.20)\n",
    "\n",
    "            \n",
    "            # Store the Axes object in our 2D array\n",
    "            axs[i, layer - 1] = ax\n",
    "\n",
    "            # Add x-label at the top\n",
    "            if i == 0:\n",
    "                ax.xaxis.set_label_position('top')\n",
    "                ax.set_xlabel(f\"Layer {layer}\", fontweight='bold')\n",
    "            \n",
    "            # Add y-label on the left\n",
    "            if layer == 0 or node_type == NodeType.RESID_END:\n",
    "                ax.set_ylabel(str(node_type.name), fontweight='bold')\n",
    "\n",
    "    # Remove empty spaces in the figure\n",
    "    fig.tight_layout()\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "    # save figure\n",
    "\n",
    "    plt.savefig(ps_dir / \"rank_corr_by_component.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # save rank correlations\n",
    "    save_json({str(k): v for k, v in rank_correlations.items()}, ps_dir, \"rank_cor_by_component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    # plot scores on x, y\n",
    "    plt.scatter(act_prune_scores_flat.cpu(), attr_prune_scores_flat.cpu(), alpha=0.25)\n",
    "    plt.xlabel(\"Act Patch Scores\")\n",
    "    plt.ylabel(\"Attrib Patch Scores\")\n",
    "    plt.xscale(\"symlog\")\n",
    "    plt.yscale(\"symlog\")\n",
    "    plt.savefig(ps_dir / \"act_attr_scores.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.prune_algo == PruneAlgo.ATTR_PATCH and act_prune_scores is not None:\n",
    "    # plot scores on x, y\n",
    "    plt.scatter(act_prune_scores_flat.abs().cpu(), attr_prune_scores_flat.abs().cpu(), alpha=0.25)\n",
    "    plt.xlabel(\"Act Patch Scores\")\n",
    "    plt.ylabel(\"Attrib Patch Scores\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.savefig(ps_dir / \"act_attr_abs_scores.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construt Circuits from Prune Scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing circuits from prune scores using either edge or fraction of prune score thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prune scores\n",
    "if conf.prune_algo == PruneAlgo.ACDC:\n",
    "    prune_scores = auto_prune_scores\n",
    "elif conf.prune_algo == PruneAlgo.CIRC_PROBE:\n",
    "    prune_scores = circ_probe_prune_scores\n",
    "elif conf.prune_algo == PruneAlgo.ACT_PATCH:\n",
    "    prune_scores = act_prune_scores\n",
    "else:\n",
    "    prune_scores = attr_prune_scores\n",
    "\n",
    "prune_scores = {mod_name: score.to(task.device) for mod_name, score in prune_scores.items()}\n",
    "# sort prune scores\n",
    "sorted_prune_scores = desc_prune_scores(prune_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot prune scores\n",
    "def plot_prune_scores(edge_scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    # plot edge scores with x labels max to 0 \n",
    "    ax.plot(sorted(edge_scores, reverse=True))\n",
    "    ax.set_xlim(len(edge_scores), 0)\n",
    "    # log axis \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(\"Edge Count\")\n",
    "    ax.set_ylabel(\"Edge Score\")\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_prune_scores(sorted_prune_scores.cpu().numpy().tolist())\n",
    "plt.savefig(ps_dir / \"edge_scores.png\")\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/n_circ_edges.json\n"
     ]
    }
   ],
   "source": [
    "# compute n_edges \n",
    "from auto_circuit.utils.graph_utils import edge_counts_util\n",
    "circ_edges = []\n",
    "if conf.prune_algo == PruneAlgo.ACDC:\n",
    "     circ_edges = [\n",
    "         t.sum(t.cat([t.flatten(v) for v in auto_prune_scores.values()]) > tao).item()\n",
    "        for tao in taos\n",
    "    ]\n",
    "     circ_thresholds = taos[1:] + [t.inf] # prune scores set to tau if change less than tau\n",
    "elif conf.prune_algo == PruneAlgo.CIRC_PROBE:\n",
    "     # get size of each circuit and thresholds \n",
    "    circ_edges = edge_counts_util(task.model.edges, prune_scores=prune_scores, zero_edges=False)\n",
    "    circ_thresholds = [sorted_prune_scores[n_edges-1].item() for n_edges in circ_edges]\n",
    "else:\n",
    "    circ_edges = edge_counts_util(task.model.edges, conf.edge_counts, zero_edges=False)\n",
    "    circ_thresholds = [sorted_prune_scores[n_edges-1].item() for n_edges in circ_edges]\n",
    "\n",
    "save_json(circ_edges, edge_dir, \"n_circ_edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faithfulness: % Loss Recovered and Equivalence Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use equivalence test from Shi et al to test a. if not equivalent and b. if equivalent\n",
    "\n",
    "We also compute: \n",
    "- mean absolute error: E[abs(score(M) - score(C))] \n",
    "(spiritually similar to Transfomer Circuits not Robust and this comment https://www.lesswrong.com/posts/kcZZAsEjwrbczxN2i/causal-scrubbing-appendix#hJoCMcgXpk8jBLvb7, we don't do fraction of recovered b/c the negatives are weird and annoying)\n",
    "- mean difference: E[score(M)] - E[score(C)] \n",
    "(kind of a middle ground, measuring bias)\n",
    "- frac mean difference recovered: E[score(C)] - E[score(A)] / E[score(M)] - E[score(A)] \n",
    "(SAE work, similar to causal scrubbing, don't need to worry about variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pruning Batch 1| 2/2 [00:00<00:00,  7.83it/s]\n",
      "Pruning Batch 1| 2/2 [00:00<00:00,  7.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# first full model outt and ablated model out\n",
    "\n",
    "with t.inference_mode():\n",
    "    model_out_train: BatchOutputs = {\n",
    "        batch.key: task.model(batch.clean)[task.model.out_slice] \n",
    "        for batch in task.train_loader\n",
    "    }\n",
    "    model_out_test: BatchOutputs = {\n",
    "        batch.key: task.model(batch.clean)[task.model.out_slice] \n",
    "        for batch in task.test_loader\n",
    "    }\n",
    "\n",
    "ablated_out_train: BatchOutputs = run_fully_ablated_model(\n",
    "    model=task.model,\n",
    "    dataloader=task.train_loader,\n",
    "    ablation_type=conf.ablation_type,\n",
    ")\n",
    "\n",
    "ablated_out_test: BatchOutputs = run_fully_ablated_model(\n",
    "    model=task.model,\n",
    "    dataloader=task.test_loader,\n",
    "    ablation_type=conf.ablation_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pruning Batch 1| 2/2 [00:01<00:00,  1.32it/s]\n",
      "Pruning Batch 1| 2/2 [00:01<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# next get circuit outs for each threshold\n",
    "from auto_circuit.prune import run_circuits\n",
    "from auto_circuit.types import CircuitOutputs, PatchType\n",
    "circuit_outs_train: CircuitOutputs = run_circuits(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    test_edge_counts=circ_edges,\n",
    "    patch_type=PatchType.TREE_PATCH, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    reverse_clean_corrupt=False, \n",
    ")\n",
    "circuit_outs_train = dict(circuit_outs_train)\n",
    "\n",
    "circuit_outs_test: CircuitOutputs = run_circuits(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    test_edge_counts=circ_edges,\n",
    "    patch_type=PatchType.TREE_PATCH, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    reverse_clean_corrupt=False, \n",
    ")\n",
    "circuit_outs_test = dict(circuit_outs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mae: E[abs(score(M) - score(C))] \n",
    "- mean difference: E[score(M)] - E[score(C)] \n",
    "- frac mean difference recovered: E[score(C)] - E[score(A)] / E[score(M)] - E[score(A)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3406728206934413423, 8911613281540895698],\n",
       " [3406728206934413423, 8911613281540895698])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[batch.key for batch in task.train_loader], [k for k in model_out_train.keys()], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3406728206934413423, 8911613281540895698],\n",
       " [3406728206934413423, 8911613281540895698],\n",
       " [3406728206934413423, 8911613281540895698],\n",
       " [3406728206934413423, 8911613281540895698],\n",
       " [3406728206934413423, 8911613281540895698],\n",
       " [3406728206934413423, 8911613281540895698],\n",
       " [3406728206934413423, 8911613281540895698],\n",
       " [3406728206934413423, 8911613281540895698]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[k for k in out.keys()] for out in circuit_outs_train.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing faith metrics| 8/8 [00:00<00:00, 161.83it/s]\n",
      "Computing faith metrics| 8/8 [00:00<00:00, 151.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metric_results_train.json\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metrics_train.json\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metric_results_test.json\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metrics_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "faith_metric_results_train, faith_metrics_train = compute_faith_metrics(\n",
    "    task.train_loader,\n",
    "    model_out_train,\n",
    "    ablated_out_train,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    "    circs_outs=circuit_outs_train,\n",
    ")\n",
    "\n",
    "faith_metric_results_test, faith_metrics_test = compute_faith_metrics(\n",
    "    task.test_loader,\n",
    "    model_out_test,\n",
    "    ablated_out_test,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    "    circs_outs=circuit_outs_test,\n",
    ")\n",
    "\n",
    "save_json(faith_metric_results_train, edge_dir, \"faith_metric_results_train\")\n",
    "save_json(faith_metrics_train, edge_dir, \"faith_metrics_train\")\n",
    "save_json(faith_metric_results_test, edge_dir, \"faith_metric_results_test\")\n",
    "save_json(faith_metrics_test, edge_dir, \"faith_metrics_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing faith metrics| 8/8 [00:00<00:00, 224.97it/s]\n",
      "Computing faith metrics| 8/8 [00:00<00:00, 224.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metric_results_train_eval.json\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metrics_train_eval.json\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metric_results_test_eval.json\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metrics_test_eval.json\n"
     ]
    }
   ],
   "source": [
    "# faith metrics eval \n",
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    faith_metric_results_train_eval, faith_metrics_train_eval = compute_faith_metrics(\n",
    "        task.train_loader,\n",
    "        model_out_train,\n",
    "        ablated_out_train,\n",
    "        conf.eval_grad_func,\n",
    "        conf.eval_answer_func,\n",
    "        circs_outs=circuit_outs_train,\n",
    "    )\n",
    "\n",
    "    faith_metric_results_test_eval, faith_metrics_test_eval = compute_faith_metrics(\n",
    "        task.test_loader,\n",
    "        model_out_test,\n",
    "        ablated_out_test,\n",
    "        conf.eval_grad_func,\n",
    "        conf.eval_answer_func,\n",
    "        circs_outs=circuit_outs_test,\n",
    "    )\n",
    "    save_json(faith_metric_results_train_eval, edge_dir, \"faith_metric_results_train_eval\")\n",
    "    save_json(faith_metrics_train_eval, edge_dir, \"faith_metrics_train_eval\")\n",
    "    save_json(faith_metric_results_test_eval, edge_dir, \"faith_metric_results_test_eval\")\n",
    "    save_json(faith_metrics_test_eval, edge_dir, \"faith_metrics_test_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalence Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/equiv_test_results_train.json\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/equiv_test_results_test.json\n"
     ]
    }
   ],
   "source": [
    "use_eval_metrics = conf.answer_func in DIV_ANSWER_FUNCS\n",
    "equiv_test_results_train = equiv_tests(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    grad_func=conf.grad_func if not use_eval_metrics else conf.eval_grad_func,\n",
    "    answer_func=conf.answer_func if not use_eval_metrics else conf.eval_answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    model_out=model_out_train,\n",
    "    circuit_outs=circuit_outs_train,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.epsilon,\n",
    ")\n",
    "\n",
    "equiv_test_results_test = equiv_tests(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    grad_func=conf.grad_func if not use_eval_metrics else conf.eval_grad_func,\n",
    "    answer_func=conf.answer_func if not use_eval_metrics else conf.eval_answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    model_out=model_out_test,\n",
    "    circuit_outs=circuit_outs_test,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.epsilon,\n",
    ")\n",
    "\n",
    "save_json(equiv_test_results_train, edge_dir, \"equiv_test_results_train\")\n",
    "save_json(equiv_test_results_test, edge_dir, \"equiv_test_results_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Sufficiency Test, and Expected Loss Recovered with Respect to Expected Value of Random Circuit of the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot % loss recovered and Equiv Test Results Along Frac Edges / Frac Prune Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "from auto_circuit_tests.hypo_tests.equiv_test import EquivResult\n",
    "from auto_circuit_tests.hypo_tests.indep_test import IndepResult\n",
    "from typing import cast\n",
    "\n",
    "def plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results: Dict[int, Dict[str, float]], \n",
    "    equiv_test_results: Optional[Dict[int, Union[EquivResult, IndepResult]]]=None,\n",
    "    title: str = \"\", \n",
    "    result_type: Optional[Literal[\"equiv\", \"indep\"]] = \"equiv\",\n",
    "    x_label: str = \"Edges\"\n",
    "):\n",
    "    n_edges = list(faith_metric_results.keys())\n",
    "    fracs = [n_edge / task.model.n_edges for n_edge in n_edges]\n",
    "    frac_loss_recovered = [faith_metric_results[n_edge][\"frac_mean_diff_recovered\"] for n_edge in n_edges]\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot line\n",
    "    ax.plot(fracs, frac_loss_recovered, color='blue')\n",
    "\n",
    "    good_color = 'green'\n",
    "    bad_color = 'blue'\n",
    "\n",
    "    # color points based on hypothesis test results\n",
    "    if result_type == \"equiv\":\n",
    "        equiv_test_results = cast(Dict[int, EquivResult], equiv_test_results)\n",
    "        reject_equivs = [result.reject_equiv for result in equiv_test_results.values()]\n",
    "        reject_non_equivs = [result.reject_non_equiv for result in equiv_test_results.values()]\n",
    "        for frac, loss, reject_equiv, reject_non_equiv in zip(fracs, frac_loss_recovered, reject_equivs,reject_non_equivs):\n",
    "            color = 'light' + good_color if reject_non_equiv else (good_color if not reject_equiv else bad_color)\n",
    "            ax.scatter(frac, loss, color=color, s=100, zorder=5)  # s is the size of the dot, zorder ensures it's on top\n",
    "        \n",
    "        ax.scatter([], [], color='light' + good_color, label='Non-Equiv Rejected', s=100)\n",
    "        ax.scatter([], [], color=good_color, label='Equiv Not Rejected', s=100)\n",
    "        ax.scatter([], [], color=bad_color, label='Equiv Rejected', s=100)\n",
    "        ax.legend()\n",
    "    elif result_type == \"indep\": \n",
    "        indep_test_results = cast(Dict[int, IndepResult], equiv_test_results)\n",
    "        reject_indeps = [result.reject_null for result in indep_test_results.values()]\n",
    "        for frac, loss, reject_indep in zip(fracs, frac_loss_recovered, reject_indeps):\n",
    "            color = 'light' + good_color if not reject_indep else bad_color\n",
    "            ax.scatter(frac, loss, color=color, s=100, zorder=5)\n",
    "        \n",
    "        ax.scatter([], [], color=good_color, label='Indep Not Rejected', s=100)\n",
    "        ax.scatter([], [], color=bad_color, label='Indep Rejected', s=100)\n",
    "        \n",
    "\n",
    "    ax.set_xlabel(f\"Fraction of Total {x_label}\")\n",
    "    ax.set_ylabel(\"Fraction of Loss Recovered\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # horizontal line at 0.95\n",
    "    ax.axhline(0.95, color='r', linestyle='--')\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    x_ticks = np.arange(0, 1.0, 0.1)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels([f'{x:.1f}' for x in x_ticks])\n",
    "\n",
    "    # Set x-axis limits\n",
    "    ax.set_xlim(0, 1.0)\n",
    "\n",
    "\n",
    "    # Adjust layout to prevent clipping of labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results using div answer function (no equivalence results)\n",
    "if use_eval_metrics:\n",
    "    # TODO: figure out why this plotting is off\n",
    "    fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "        faith_metric_results_train,\n",
    "        title=\"(Train) Fraction of Loss Recovered\",\n",
    "        result_type=None,\n",
    "        x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    "    )\n",
    "    fig.savefig(edge_dir / \"frac_loss_recovered_results_train.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results using div answer function (no equivalence results)\n",
    "if use_eval_metrics:\n",
    "    fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "        faith_metric_results_test,\n",
    "        title=\"(Test) Fraction of Loss Recovered\",\n",
    "        result_type=None,\n",
    "        x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    "    )\n",
    "    fig.savefig(edge_dir / \"frac_loss_recovered_results_test.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out why this plotting is off\n",
    "\n",
    "fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results_train if not use_eval_metrics else faith_metric_results_train_eval, \n",
    "    equiv_test_results_train,\n",
    "    title=\"(Train) Fraction of Loss Recovered and Equiv Test Results\",\n",
    "    result_type=\"equiv\",\n",
    "    x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    ")\n",
    "fig.savefig(edge_dir / \"frac_loss_recovered_and_equiv_test_results_train.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results_test if not use_eval_metrics else faith_metric_results_test_eval, \n",
    "    equiv_test_results_test,\n",
    "    title=\"(Test) Fraction of Loss Recovered and Equiv Test Results\",\n",
    "    result_type=\"equiv\",\n",
    "    x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    ")\n",
    "fig.savefig(edge_dir / \"frac_loss_recovered_and_equiv_test_results_test.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimality of Smallest Circuit with %loss recovered > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Smallest Equivalent Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_ps = flat_prune_scores_ordered(prune_scores, order=prune_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/min_equiv_edges_train.json\n"
     ]
    }
   ],
   "source": [
    "# find smallest equiv circuit on training distribution\n",
    "edge_counts_equiv_idx = [\n",
    "    i for i, (k, v) in enumerate(equiv_test_results_train.items())\n",
    "    if faith_metric_results_train[k]['frac_mean_diff_recovered'] > 0.95 \n",
    "    #and v.reject_null \n",
    "]\n",
    "n_edges_min_equi_idx = min(edge_counts_equiv_idx) if edge_counts_equiv_idx else -1\n",
    "n_edges_min_equiv = circ_edges[n_edges_min_equi_idx]\n",
    "threshold = circ_thresholds[n_edges_min_equi_idx]\n",
    "\n",
    "# get edges of circuit\n",
    "edge_mask = {k: torch.abs(v) >= threshold for k, v in prune_scores.items()}\n",
    "edges = edges_from_mask(task.model.srcs, task.model.dests, edge_mask, task.token_circuit)\n",
    "save_json([(edge.seq_idx, edge.name) for edge in  edges], edge_dir, \"min_equiv_edges_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_task = TASK_TO_OUTPUT_ANSWER_FUNCS[conf.task] == (conf.grad_func, conf.answer_func) or conf.answer_func in DIV_ANSWER_FUNCS\n",
    "test_smallest = valid_task and len(edges) < 20_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Pruned Smallest Equivalent Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_smallest and not conf.prune_algo == PruneAlgo.ACDC:\n",
    "    fig = draw_seq_graph(\n",
    "        model=task.model,\n",
    "        prune_scores=prune_scores,\n",
    "        score_threshold=threshold,\n",
    "        show_all_seq_pos=True,\n",
    "        orientation=\"h\",\n",
    "        display_ipython=False,#is_notebook(),\n",
    "        seq_labels=task.test_loader.seq_labels,\n",
    "    )\n",
    "    fig.write_image(repo_path_to_abs_path(edge_dir / \"smallest_equiv_circ_graph_train.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Unused Edges\n",
    "\n",
    "Note: Seems like there is some leakage, not exactly sure why, but I guess its fine, not using this anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of unused edges: 0.0\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/frac_unused_edges.json\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/frac_unused_edge_scores.json\n"
     ]
    }
   ],
   "source": [
    "if test_smallest:\n",
    "    # from auto_circuit_tests.edge_graph import find_unused_edges\n",
    "    def sum_prune_scores(edges: list[Edge]) -> t.Tensor:\n",
    "        return sum([\n",
    "            torch.abs(prune_scores[edge.dest.module_name][edge.patch_idx])\n",
    "            for edge in edges\n",
    "        ])\n",
    "    # find unused edges\n",
    "    used_edges, unused_edges, _circ_graph = find_unused_edges(\n",
    "        edges, conf.ablation_type, token_circuit=task.token_circuit, attn_only=task.model.cfg.attn_only\n",
    "        )\n",
    "    # get prune scores for each unused edge \n",
    "    unused_edge_prune_scores_train = {\n",
    "        edge: prune_scores[edge.dest.module_name][edge.patch_idx]\n",
    "        for edge in unused_edges\n",
    "    }\n",
    "    # save unused edges with prune scores\n",
    "    # save_json(unused_edge_prune_scores_train, edge_dir, \"unused_edges_train\")\n",
    "    print(f\"Fraction of unused edges: {len(unused_edges) / len(edges)}\")\n",
    "    save_json(len(unused_edges) / len(edges), edge_dir, \"frac_unused_edges\")\n",
    "    # save fraction of prune scores attributed to unused edges in circuit\n",
    "    total_circuit_prune_scores = sum_prune_scores(edges)\n",
    "    unused_edge_prune_scores_abs = sum_prune_scores(unused_edges)\n",
    "    save_json((unused_edge_prune_scores_abs / total_circuit_prune_scores).item(), edge_dir, \"frac_unused_edge_scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Pruned Smallest Circuit Still Equivalent and achieves >95% loss recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pruning Batch 1| 2/2 [00:00<00:00,  9.83it/s]\n"
     ]
    }
   ],
   "source": [
    "if test_smallest:\n",
    "    from auto_circuit.types import PruneScores\n",
    "    # get prune score mask\n",
    "    def edges_to_prune_score_mask(edges: list[Edge]) -> t.Tensor:\n",
    "        mask = task.model.new_prune_scores()\n",
    "        for edge in edges:\n",
    "            mask[edge.dest.module_name][edge.patch_idx] = 1\n",
    "        return mask\n",
    "\n",
    "    # compute circuit outputs for used edges \n",
    "    def run_circuit_from_mask(\n",
    "        mask: PruneScores, \n",
    "        dataloader: PromptDataLoader,\n",
    "    ) -> CircuitOutputs:\n",
    "        circuit_out: CircuitOutputs = run_circuits(\n",
    "            model=task.model, \n",
    "            dataloader=dataloader,\n",
    "            prune_scores=mask,\n",
    "            thresholds = [0.5],\n",
    "            patch_type=PatchType.TREE_PATCH, \n",
    "            ablation_type=conf.ablation_type,\n",
    "            reverse_clean_corrupt=False, \n",
    "        )\n",
    "        return circuit_out\n",
    "\n",
    "    used_edges_mask = edges_to_prune_score_mask(used_edges)\n",
    "    used_edges_out = run_circuit_from_mask(used_edges_mask, task.train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing faith metrics| 1/1 [00:00<00:00, 144.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Edges Train %loss recovered: 0.9970371127128601\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/faith_metric_results_used_edges.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if test_smallest:\n",
    "# compute faithfulness metrics \n",
    "    faith_metric_results_used_edges, faith_metrics_used_edges = compute_faith_metrics(\n",
    "        task.train_loader,\n",
    "        model_out_train,\n",
    "        ablated_out_train,\n",
    "        conf.grad_func,\n",
    "        conf.answer_func,\n",
    "        circs_outs=used_edges_out,\n",
    "    )\n",
    "    print(f\"Used Edges Train %loss recovered: {list(faith_metric_results_used_edges.values())[0]['frac_mean_diff_recovered']}\")\n",
    "    save_json(faith_metric_results_used_edges, edge_dir, \"faith_metric_results_used_edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Edges Null Rejected: False\n",
      "Saving json to /nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Token_Circuit/RESAMPLE/LOGPROB_KL_DIV/acdc/tao/equiv_test_results_used_edges.json\n"
     ]
    }
   ],
   "source": [
    "# run equiv tests on used edges\n",
    "if test_smallest:\n",
    "    equiv_test_results_used_edges = equiv_tests(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader,\n",
    "        prune_scores=used_edges_mask,\n",
    "        grad_func=conf.grad_func,\n",
    "        answer_func=conf.answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        model_out=model_out_train,\n",
    "        circuit_outs=used_edges_out,\n",
    "        alpha=conf.alpha,\n",
    "        epsilon=conf.epsilon,\n",
    "    )\n",
    "    print(f\"Used Edges Null Rejected: {list(equiv_test_results_used_edges.values())[0].reject_non_equiv}\")\n",
    "    save_json(equiv_test_results_used_edges, edge_dir, \"equiv_test_results_used_edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimality Test and Change in %loss Recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run on docstring to save time\n",
    "run_min_test = test_smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Scores after  Ablating Each Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "          | 0/3699 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 1/3699 [00:00<17:17,  3.56it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 2/3699 [00:00<16:46,  3.67it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 3/3699 [00:00<17:09,  3.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 4/3699 [00:01<16:58,  3.63it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 5/3699 [00:01<19:01,  3.24it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 6/3699 [00:01<18:33,  3.32it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 7/3699 [00:02<18:16,  3.37it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 8/3699 [00:02<18:04,  3.40it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 9/3699 [00:02<17:36,  3.49it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 10/3699 [00:02<17:59,  3.42it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 11/3699 [00:03<17:43,  3.47it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 12/3699 [00:03<17:36,  3.49it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 13/3699 [00:03<17:43,  3.47it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 14/3699 [00:04<17:37,  3.49it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 15/3699 [00:04<17:41,  3.47it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 16/3699 [00:04<17:49,  3.44it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 17/3699 [00:04<17:52,  3.43it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 18/3699 [00:05<18:01,  3.40it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 19/3699 [00:05<18:03,  3.39it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 20/3699 [00:05<18:11,  3.37it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 21/3699 [00:06<18:10,  3.37it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 22/3699 [00:06<18:23,  3.33it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 23/3699 [00:06<18:23,  3.33it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 24/3699 [00:07<18:24,  3.33it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 25/3699 [00:07<18:26,  3.32it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 26/3699 [00:07<18:18,  3.35it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 27/3699 [00:07<18:14,  3.35it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 28/3699 [00:08<17:59,  3.40it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 29/3699 [00:08<17:28,  3.50it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 30/3699 [00:08<17:07,  3.57it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 31/3699 [00:09<16:47,  3.64it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 32/3699 [00:09<16:29,  3.71it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 33/3699 [00:09<16:20,  3.74it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 34/3699 [00:09<16:12,  3.77it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 35/3699 [00:10<16:11,  3.77it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 36/3699 [00:10<16:14,  3.76it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 37/3699 [00:10<16:26,  3.71it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 38/3699 [00:10<16:30,  3.70it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 39/3699 [00:11<16:36,  3.67it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 40/3699 [00:11<17:19,  3.52it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 41/3699 [00:11<17:31,  3.48it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 42/3699 [00:12<18:05,  3.37it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 43/3699 [00:12<17:57,  3.39it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 44/3699 [00:12<17:39,  3.45it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 45/3699 [00:12<17:14,  3.53it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "          | 46/3699 [00:13<17:20,  3.51it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 47/3699 [00:13<17:43,  3.43it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 48/3699 [00:13<17:52,  3.40it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 49/3699 [00:14<18:02,  3.37it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 50/3699 [00:14<18:10,  3.35it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 51/3699 [00:14<18:06,  3.36it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 52/3699 [00:15<18:09,  3.35it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 53/3699 [00:15<18:16,  3.33it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 54/3699 [00:15<18:23,  3.30it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 55/3699 [00:15<18:08,  3.35it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 56/3699 [00:16<18:03,  3.36it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 57/3699 [00:16<18:06,  3.35it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 58/3699 [00:16<18:08,  3.35it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 59/3699 [00:17<18:08,  3.34it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 60/3699 [00:17<18:12,  3.33it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 61/3699 [00:17<18:19,  3.31it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 62/3699 [00:18<18:19,  3.31it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 63/3699 [00:18<17:45,  3.41it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 64/3699 [00:18<17:08,  3.53it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 65/3699 [00:18<16:46,  3.61it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "         | 65/3699 [00:19<17:46,  3.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_min_test:\n\u001b[0;32m----> 2\u001b[0m     edges_scores_train \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_edge_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprune_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprune_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mablation_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mablation_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_out_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     edges_scores_test \u001b[38;5;241m=\u001b[39m compute_edge_scores(\n\u001b[1;32m     15\u001b[0m         model\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mtest_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[1;32m     24\u001b[0m     )\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/auto-circuit-tests/auto_circuit_tests/edge_scores.py:35\u001b[0m, in \u001b[0;36mcompute_edge_scores\u001b[0;34m(model, dataloader, prune_scores, edges, grad_func, answer_func, ablation_type, model_out, threshold)\u001b[0m\n\u001b[1;32m     25\u001b[0m     edge_outs \u001b[38;5;241m=\u001b[39m run_circuit_with_edge_ablated(\n\u001b[1;32m     26\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m     27\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mdataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m         to_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# comute scores \u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     edge_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m         batch\u001b[38;5;241m.\u001b[39mkey: score_func(edge_outs[batch\u001b[38;5;241m.\u001b[39mkey], batch, model_out[batch\u001b[38;5;241m.\u001b[39mkey]) \n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader\n\u001b[1;32m     38\u001b[0m     }\n\u001b[1;32m     39\u001b[0m     edges_scores[edge] \u001b[38;5;241m=\u001b[39m edge_scores\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edges_scores\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/auto-circuit-tests/auto_circuit_tests/edge_scores.py:35\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m     edge_outs \u001b[38;5;241m=\u001b[39m run_circuit_with_edge_ablated(\n\u001b[1;32m     26\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m     27\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mdataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m         to_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# comute scores \u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     edge_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m         batch\u001b[38;5;241m.\u001b[39mkey: score_func(edge_outs[batch\u001b[38;5;241m.\u001b[39mkey], batch, model_out[batch\u001b[38;5;241m.\u001b[39mkey]) \n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader\n\u001b[1;32m     38\u001b[0m     }\n\u001b[1;32m     39\u001b[0m     edges_scores[edge] \u001b[38;5;241m=\u001b[39m edge_scores\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edges_scores\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/ucb/oliveradk/auto-circuit-tests/auto-circuit/auto_circuit/data.py:91\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Sometimes each prompt has a different number of wrong answers\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     wrong_answers \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mwrong_answers \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m---> 91\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhash\u001b[39m((\u001b[38;5;28mstr\u001b[39m(\u001b[43mclean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28mstr\u001b[39m(corrupt\u001b[38;5;241m.\u001b[39mtolist())))\n\u001b[1;32m     93\u001b[0m diverge_idxs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m~\u001b[39m(clean \u001b[38;5;241m==\u001b[39m corrupt))\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     94\u001b[0m batch_dvrg_idx: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(diverge_idxs\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if run_min_test:\n",
    "    edges_scores_train = compute_edge_scores(\n",
    "        model=task.model,\n",
    "        dataloader=task.train_loader,\n",
    "        prune_scores=prune_scores,\n",
    "        edges=edges,\n",
    "        grad_func=conf.grad_func,\n",
    "        answer_func=conf.answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        model_out=model_out_train,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    edges_scores_test = compute_edge_scores(\n",
    "        model=task.model,\n",
    "        dataloader=task.test_loader,\n",
    "        prune_scores=prune_scores,\n",
    "        edges=edges,\n",
    "        grad_func=conf.grad_func,\n",
    "        answer_func=conf.answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        model_out=model_out_test,\n",
    "        threshold=threshold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Change in %loss recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    edge_faith_metric_results_train, edge_faith_metrics_train = compute_faith_metrics(\n",
    "        task.train_loader,\n",
    "        model_out_train,\n",
    "        ablated_out_train,\n",
    "        conf.grad_func,\n",
    "        conf.answer_func,\n",
    "        circs_scores=edges_scores_train,\n",
    "    )\n",
    "    # hmm this should just be by edge, also I want the edge order\n",
    "    save_json({edge_name(k): v for k, v in edge_faith_metric_results_train.items()}, edge_dir, \"edge_faith_metric_results_train\")\n",
    "    save_json({edge_name(k): v for k, v in edge_faith_metrics_train.items()}, edge_dir, \"edge_faith_metrics_train\")\n",
    "\n",
    "    edge_faith_metric_results_test, edge_faith_metrics_test = compute_faith_metrics(\n",
    "        task.test_loader,\n",
    "        model_out_test,\n",
    "        ablated_out_test,\n",
    "        conf.grad_func,\n",
    "        conf.answer_func,\n",
    "        circs_scores=edges_scores_test,\n",
    "    )\n",
    "    save_json({edge_name(k): v for k, v in edge_faith_metric_results_test.items()}, edge_dir, \"edge_faith_metric_results_test\")\n",
    "    save_json({edge_name(k): v for k, v in edge_faith_metrics_test.items()}, edge_dir, \"edge_faith_metrics_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # plot change in loss recovered \n",
    "    frac_loss_recovered_train = faith_metric_results_train[n_edges_min_equiv]['frac_mean_diff_recovered']\n",
    "    frac_loss_recovered_test = faith_metric_results_test[n_edges_min_equiv]['frac_mean_diff_recovered']\n",
    "    # sort edges by prune scores \n",
    "    edge_prune_scores = {\n",
    "        edge: prune_scores[edge.dest.module_name][edge.patch_idx].cpu().item()\n",
    "        for edge in edges\n",
    "    }\n",
    "    sorted_edge_prune_scores = sorted(edge_prune_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    frac_loss_recovered_train_sorted = [edge_faith_metric_results_train[edge]['frac_mean_diff_recovered'] for edge, _ in sorted_edge_prune_scores]\n",
    "    frac_loss_recovered_test_sorted = [edge_faith_metric_results_test[edge]['frac_mean_diff_recovered'] for edge, _ in sorted_edge_prune_scores]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # add transparency to lines\n",
    "    ax.plot([frac_loss_recovered_train - x for x in reversed(frac_loss_recovered_train_sorted)], label=\"Train\")\n",
    "    ax.plot([frac_loss_recovered_test - x for x in reversed(frac_loss_recovered_test_sorted)], label=\"Test\", alpha=0.75)\n",
    "    # horizontal line at 1/circuit_size\n",
    "    ax.axhline((1/ len(edges)) * 100, color='r', linestyle='--')\n",
    "\n",
    "    ax.set_xlabel(\"Edge Index\")\n",
    "    ax.set_ylabel(\"Change in Fraction of Loss Recovered\")\n",
    "    ax.set_title(\"Change in Fraction of Loss Recovered for Each Edge\")\n",
    "\n",
    "    # add legend\n",
    "    ax.legend()\n",
    "\n",
    "    plt.savefig(edge_dir / \"frac_loss_recovered_change.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Activation Patching Scores on Circuit and Correlation Between Activation Patching Scores on Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    from auto_circuit.prune_algos.utils import compute_loss \n",
    "    def compute_full_model_score(\n",
    "        model: PatchableModel,\n",
    "        dataloader: PromptDataLoader,\n",
    "        model_outs: BatchOutputs,\n",
    "        grad_func: str,\n",
    "        answer_func: str,\n",
    "    ):\n",
    "        full_model_score = 0\n",
    "        for batch in dataloader:\n",
    "            full_model_score -= compute_loss(\n",
    "                model, \n",
    "                batch, \n",
    "                grad_func, \n",
    "                answer_func, \n",
    "                logits=model_outs[batch.key].to(task.device)\n",
    "            ).sum().item()\n",
    "        return full_model_score\n",
    "\n",
    "    full_model_score = 0\n",
    "    if conf.answer_func not in DIV_ANSWER_FUNCS:\n",
    "        full_model_score = compute_full_model_score(\n",
    "            task.model,\n",
    "            task.train_loader, \n",
    "            model_out_train, \n",
    "            conf.grad_func.value, \n",
    "            conf.answer_func.value\n",
    "        )\n",
    "\n",
    "    def compute_edge_act_prune_scores(\n",
    "        model: PatchableModel,\n",
    "        edges_scores: dict[Edge, BatchOutputs],\n",
    "        full_model_score: float,\n",
    "    ) -> PruneScores:\n",
    "        edge_prune_scores = model.new_prune_scores()\n",
    "        for mod_name in edge_prune_scores.keys():\n",
    "            edge_prune_scores[mod_name] += full_model_score\n",
    "        for edge, edge_scores in edges_scores.items():\n",
    "                edge_scores = t.cat([v for v in edge_scores.values()])\n",
    "                edge_prune_scores[edge.dest.module_name][edge.patch_idx] -= edge_scores.sum()\n",
    "        return edge_prune_scores\n",
    "\n",
    "\n",
    "    # edge outs train \n",
    "    edge_circ_act_prune_scores = compute_edge_act_prune_scores(\n",
    "        task.model,\n",
    "        edges_scores_train,\n",
    "        full_model_score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation\n",
    "# TODO: compute actual activation patching scores for circuit, compute actual correlation\n",
    "if run_min_test:\n",
    "    # plot correlation between minimality score (change in score) and prune score \n",
    "    edge_act_prune_scores_flat = [act_prune_scores[edge.dest.module_name][edge.patch_idx].item() for edge in edges]\n",
    "    edge_circ_act_prune_scores_flat = [edge_circ_act_prune_scores[edge.dest.module_name][edge.patch_idx].item() for edge in edges]\n",
    "    \n",
    "    # compute correlation coefficient \n",
    "    from scipy import stats\n",
    "    corr, p_value = stats.pearsonr(edge_act_prune_scores_flat, edge_circ_act_prune_scores_flat)\n",
    "    \n",
    "    plt.scatter(edge_act_prune_scores_flat, edge_circ_act_prune_scores_flat, s=1.0)\n",
    "    plt.xlabel(\"Act Patch Scores\")\n",
    "    plt.ylabel(\"Circuit Act Patch Scores\")\n",
    "    plt.title(f\"Correlation: {corr:.2f}, p-value: {p_value:.2f}\")\n",
    "    plt.savefig(edge_dir / \"circ_act_patch_corr.png\")\n",
    "    plt.close()\n",
    "\n",
    "    save_json({\"corr\": corr, \"p_value\": p_value}, edge_dir, \"circ_act_patch_corr_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # build full grap to sample paths\n",
    "    graph = SeqGraph(task.model.edges, token=task.token_circuit, attn_only=task.model.cfg.attn_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # ok so there should be columns for each sequence position, and subcolumsn for each component\n",
    "    seq_idxs = [0, task.test_loader.seq_len-1] if task.token_circuit else None\n",
    "    if is_notebook():\n",
    "        visualize_graph(graph, sort_by_head=False, max_layer=None, seq_idxs=seq_idxs, column_width=5, figsize=(36, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot circuit graph\n",
    "if run_min_test:\n",
    "    circ_graph = SeqGraph(edges, token=task.token_circuit, attn_only=task.model.cfg.attn_only)\n",
    "    seq_idxs = set([seq_node.seq_idx for seq_node in circ_graph.seq_nodes])\n",
    "    if is_notebook():\n",
    "        visualize_graph(circ_graph, sort_by_head=False, max_layer=None, seq_idxs=seq_idxs, column_width=10, figsize=(72, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample paths from complement for each data instance\n",
    "if run_min_test:\n",
    "    complement_edges = set(task.model.edges) - set(edges)\n",
    "    sampled_paths = sample_paths(\n",
    "        seq_graph=graph, \n",
    "        n_paths=conf.n_paths, \n",
    "        complement_edges=complement_edges,\n",
    "    )\n",
    "    edges_set = set(edges)\n",
    "    novel_edge_paths = [[edge for edge in path if edge not in edges_set] for path in sampled_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    path_idx = 0\n",
    "    sampled_path = sampled_paths[path_idx]\n",
    "    novel_edges = novel_edge_paths[path_idx]\n",
    "    redundant_edges = set(sampled_path).intersection(set(edges))\n",
    "    print(f\"Added edges: {novel_edges}\")\n",
    "    print(f\"Redundant edges: {redundant_edges}\")\n",
    "    ex_inflated_graph = SeqGraph(edges + list(novel_edges), token=task.token_circuit, attn_only=task.model.cfg.attn_only)\n",
    "    seq_idxs = set([seq_node.seq_idx for seq_node in ex_inflated_graph.seq_nodes]) if task.token_circuit else None\n",
    "    edge_colors = {}\n",
    "    [edge_colors.update({edge: 'blue'}) for edge in novel_edges]\n",
    "    [edge_colors.update({edge: 'darkblue'}) for edge in redundant_edges]\n",
    "    if is_notebook():\n",
    "        visualize_graph(ex_inflated_graph, sort_by_head=False, max_layer=None, seq_idxs=seq_idxs, edge_colors=edge_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample paths to remove \n",
    "if run_min_test:\n",
    "    ablated_paths, removed_edges = [], []\n",
    "    for path in novel_edge_paths:\n",
    "        edge_idx_to_remove = random.choice(range(len(path)))\n",
    "        ablated_path = remove_el(path, edge_idx_to_remove)\n",
    "        ablated_paths.append(ablated_path)\n",
    "        removed_edges.append(path[edge_idx_to_remove])\n",
    "    removed_edge = removed_edges[path_idx]\n",
    "    edge_colors[removed_edge] = 'red'\n",
    "    if is_notebook():\n",
    "        visualize_graph(ex_inflated_graph, sort_by_head=False, max_layer=None, seq_idxs=seq_idxs, edge_colors=edge_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    inflated_outs, ablated_outs = run_circuits_inflated_ablated(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        edges=edges,\n",
    "        n_paths=conf.n_paths,\n",
    "        graph=graph,\n",
    "        paths=sampled_paths,\n",
    "        ablated_paths=ablated_paths,\n",
    "        token=task.token_circuit,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # compute mean diffs for each inflated circuit / ablated circuit\n",
    "    inflated_ablated_mean_diffs: list[float] = []\n",
    "    for i, inflated_out in inflated_outs.items():\n",
    "        inflated_ablated_diffs = score_diffs(\n",
    "            dataloader=task.train_loader,\n",
    "            grad_func=conf.grad_func,\n",
    "            answer_func=conf.answer_func,\n",
    "            outs_1=inflated_out,\n",
    "            outs_2=ablated_outs[i],\n",
    "            model_outs=model_out_train,\n",
    "            device=task.device\n",
    "        )\n",
    "        inflated_ablated_mean_diffs.append(inflated_ablated_diffs.mean().item())\n",
    "\n",
    "    # compute mean diffs for each ablated edge\n",
    "    ablated_edge_mean_diffs: dict[Edge, float] = {}\n",
    "    for edge in edges:\n",
    "        ablated_diffs = score_diffs(\n",
    "            dataloader=task.train_loader,\n",
    "            scores_1=edges_scores_train[edge],\n",
    "            outs_2=circuit_outs_train[n_edges_min_equiv],\n",
    "            grad_func=conf.grad_func,\n",
    "            answer_func=conf.answer_func,\n",
    "            model_outs=model_out_train,\n",
    "            device=task.device\n",
    "        )\n",
    "        ablated_edge_mean_diffs[edge] = ablated_diffs.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit_tests.hypo_tests.minimality_test import MinResult\n",
    "if run_min_test:\n",
    "    min_results_train: dict[Edge, MinResult] = {}\n",
    "    for edge in tqdm(edges):\n",
    "        min_results_train[edge] = minimality_test_edge(\n",
    "            ablated_edge_mean_diff=ablated_edge_mean_diffs[edge],\n",
    "            inflated_ablated_mean_diffs=inflated_ablated_mean_diffs,\n",
    "            n_edges=len(edges),\n",
    "            alpha=conf.alpha, # bonferroni handled internally\n",
    "            q_star=conf.q_star,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_min_test:\n",
    "    # plot minimality scores and fraction of loss recovered sorted by minimality score with threshold for rejection (from paper)\n",
    "    edges_by_min_score = sorted(edges, key=lambda edge: ablated_edge_mean_diffs[edge], reverse=False)\n",
    "    min_scores = [ablated_edge_mean_diffs[edge] for edge in edges_by_min_score]\n",
    "    frac_loss_recovered_train = faith_metric_results_train[n_edges_min_equiv]['frac_mean_diff_recovered']\n",
    "    frac_loss_recovered_delta = [frac_loss_recovered_train - edge_faith_metric_results_train[edge]['frac_mean_diff_recovered'] for edge in edges_by_min_score]\n",
    "    \n",
    "    # get first edge which does not reject miniamlity, and first edge that rejects non-minimality\n",
    "    first_min_not_rejected = [min_results_train[edge].reject_min_null for edge in edges_by_min_score].index(False)\n",
    "    first_not_min_rejected = [min_results_train[edge].reject_null_non_min for edge in edges_by_min_score].index(True)\n",
    "\n",
    "    # plot minimality scores and fraction of loss recovered\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(min_scores, label=\"Change in Score\")\n",
    "    ax.set_xlabel(\"Edge Index\")\n",
    "    ax.set_ylabel(\"Change in Score\")\n",
    "    # ax.set_yscale('log')\n",
    "    # new axis for fraction of loss recovered\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(frac_loss_recovered_delta, label=\"Change in Fraction of Loss Recovered\", color='orange', alpha=0.75)\n",
    "    ax2.set_ylabel(\"Change in Fraction of Loss Recovered\")\n",
    "\n",
    "    # add vertical line and shaed region for first edge that does not reject minimality\n",
    "    ax.axvline(first_min_not_rejected, color='blue', linestyle='--')\n",
    "    ax.axvspan(0,first_min_not_rejected, color='lightblue', alpha=0.5)\n",
    "    # add veritical line and shaded region for first edge that rejects non-minimality\n",
    "    ax.axvline(first_not_min_rejected, color='green', linestyle='--')\n",
    "    ax.axvspan(first_min_not_rejected, first_not_min_rejected, color='lightgreen', alpha=0.5)\n",
    "    # TODO: put fig legend where ax legend would be \n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.15, 0.95))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_min_test:\n",
    "#     min_results_test, null_rejected_test = minimality_test(\n",
    "#         model=task.model, \n",
    "#         dataloader=task.test_loader,\n",
    "#         edges=edges,\n",
    "#         prune_scores=prune_scores,\n",
    "#         threshold=threshold,\n",
    "#         grad_func=conf.grad_func,\n",
    "#         answer_func=conf.answer_func,\n",
    "#         ablation_type=conf.ablation_type,\n",
    "#         token=task.token_circuit,\n",
    "#         model_outs=model_out_test,\n",
    "#         n_paths=conf.n_paths,\n",
    "#         q_star=conf.q_star,\n",
    "#         device=task.device,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimality Test on \"Ground Truth\" Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TASK_TO_OUTPUT_ANSWER_FUNCS[task.key] == (conf.grad_func, conf.answer_func):\n",
    "#     # inflated ablated \n",
    "#     inflated_outs_true, ablated_outs_true = run_circuits_inflated_ablated(\n",
    "#         model=task.model,\n",
    "#         dataloader=task.test_loader,\n",
    "#         ablation_type=conf.ablation_type,\n",
    "#         edges=edges,\n",
    "#         n_paths=conf.n_paths,\n",
    "#         token=task.token_circuit\n",
    "#     )\n",
    "\n",
    "#     # compute mean diffs for each inflated circuit / ablated circuit\n",
    "#     inflated_ablated_mean_diffs_true: list[float] = []\n",
    "#     for i, inflated_out in inflated_outs_true.items():\n",
    "#         inflated_ablated_diffs = score_diffs(\n",
    "#             dataloader=task.test_loader,\n",
    "#             outs_1=inflated_out,\n",
    "#             outs_2=ablated_outs_true[i],\n",
    "#             grad_func=conf.grad_func,\n",
    "#             answer_func=conf.answer_func,\n",
    "#             model_outs=model_out_test,\n",
    "#             device=task.device\n",
    "#         )\n",
    "#         inflated_ablated_mean_diffs_true.append(inflated_ablated_diffs.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_edges_min_test_path = out_answer_dir / \"true_edges_min_test_results.json\"\n",
    "# if TASK_TO_OUTPUT_ANSWER_FUNCS[task.key] == (conf.grad_func, conf.answer_func) and not true_edges_min_test_path.exists():\n",
    "#     true_edges_min_test_results, null_rejected = minimality_test(\n",
    "#         model=task.model, \n",
    "#         dataloader=task.test_loader,\n",
    "#         edges=list(task.true_edges),\n",
    "#         prune_scores=task.model.circuit_prune_scores(task.true_edges),\n",
    "#         threshold=0.5,\n",
    "#         grad_func=conf.grad_func,\n",
    "#         answer_func=conf.answer_func,\n",
    "#         ablation_type=conf.ablation_type,\n",
    "#         token=task.token_circuit,\n",
    "#         inflated_outs=inflated_outs_true,\n",
    "#         ablated_outs=ablated_outs_true,\n",
    "#         q_star=conf.q_star,\n",
    "#         device=task.device,\n",
    "#         stop_if_reject=True\n",
    "#     )\n",
    "#     save_json({edge_name(k): v for k, v in true_edges_min_test_results.items()}, out_answer_dir, \"true_edges_min_test_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence Test and Complement %Loss Recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % Loss Recovered of Complement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complement outs\n",
    "complement_outs_train = run_circuits(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    test_edge_counts=circ_edges,\n",
    "    patch_type=PatchType.EDGE_PATCH, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    reverse_clean_corrupt=True, # ablated edges are corrupt \n",
    ")\n",
    "\n",
    "complement_outs_test: CircuitOutputs = run_circuits(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    test_edge_counts=circ_edges,\n",
    "    patch_type=PatchType.EDGE_PATCH, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    reverse_clean_corrupt=True, # ablated edges are corrupt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get faithfulness metrics of complement\n",
    "faith_metric_results_c_train, faith_metrics_c_train = compute_faith_metrics(\n",
    "    task.train_loader,\n",
    "    model_out_train, \n",
    "    ablated_out_train,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    "    circs_outs=complement_outs_train,\n",
    ")\n",
    "\n",
    "\n",
    "faith_metric_results_c_test, faith_metrics_c_test = compute_faith_metrics(\n",
    "    task.test_loader,\n",
    "    model_out_test,\n",
    "    ablated_out_test,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    "    circs_outs=complement_outs_test,\n",
    ")\n",
    "\n",
    "\n",
    "save_json(faith_metric_results_c_train, edge_dir, \"faith_metric_results_c_train\")\n",
    "save_json(faith_metrics_c_train, edge_dir, \"faith_metrics_c_train\")\n",
    "save_json(faith_metric_results_c_test, edge_dir, \"faith_metric_results_c_test\")\n",
    "save_json(faith_metrics_c_test, edge_dir, \"faith_metrics_c_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute faithfulness metrics using eval functions\n",
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    faith_metric_results_c_train_eval, faith_metrics_c_train_eval = compute_faith_metrics(\n",
    "        task.train_loader,\n",
    "        model_out_train,\n",
    "        ablated_out_train,\n",
    "        conf.eval_grad_func,\n",
    "        conf.eval_answer_func,\n",
    "        circs_outs=complement_outs_train,\n",
    "    )\n",
    "\n",
    "    faith_metric_results_c_test_eval, faith_metrics_c_test_eval = compute_faith_metrics(\n",
    "        task.test_loader,\n",
    "        model_out_test,\n",
    "        ablated_out_test,\n",
    "        conf.eval_grad_func,\n",
    "        conf.eval_answer_func,\n",
    "        circs_outs=complement_outs_test,\n",
    "    )\n",
    "    save_json(faith_metric_results_c_train_eval, edge_dir, \"faith_metric_results_c_train_eval\")\n",
    "    save_json(faith_metrics_c_train_eval, edge_dir, \"faith_metrics_c_train_eval\")\n",
    "    save_json(faith_metric_results_c_test_eval, edge_dir, \"faith_metric_results_c_test_eval\")\n",
    "    save_json(faith_metrics_c_test_eval, edge_dir, \"faith_metrics_c_test_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence HCIC (Frequentist) Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Test for completeness - if the circuit contains all the components required to perform the task, then the output of the complement should be independent of the original model\n",
    "\n",
    "$H_0$: Score of complement indepedendent of score of model\n",
    "\n",
    "Hilbert Schmdit Indepednence Criterion - non-parametric measure of independence \n",
    "\n",
    "- Background: (see https://jejjohnson.github.io/research_journal/appendix/similarity/hsic/)\n",
    "\n",
    "Intuition: the trace sums along the interaction terms on each data point, which \n",
    "we expect to be larger then other interaction terms across samples if X, and Y are \n",
    "correlated, fewer of the perumations should be greater, our p-value will be smaller, \n",
    "and thus we're more likely to reject the null\n",
    "\n",
    "\n",
    "Note: the hypothesis paper defines HCIC as  K_{x,y}K_{x,y}, but can also define it as \n",
    "{K_x}{K_y}, b/c that that equality holds in general for Cross Covariance and Auto \n",
    "Covariance \n",
    "\n",
    "The paper uses $\\rho$ = median(||score(complement) - score(model)||), based on this \n",
    "paper https://arxiv.org/pdf/1707.07269\n",
    "\n",
    "I'm not sure if we can do an interval test, because it seems like we need to assume \n",
    "a kind of uniform null - I basically don't understand the test enough\n",
    "\n",
    "I want to say something like independent only if \"p value\" between 0.5 +- epsilon \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit_tests.hypo_tests.indep_test import independence_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_results_train = independence_tests(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader, \n",
    "    prune_scores=prune_scores, \n",
    "    grad_func=conf.grad_func,\n",
    "    answer_func=conf.answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    model_out=model_out_train,\n",
    "    complement_circuit_outs=complement_outs_train,\n",
    "    alpha=conf.alpha,\n",
    "    B=1000\n",
    ")\n",
    "save_json(indep_results_train, edge_dir, \"indep_results_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_results_test = independence_tests(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader, \n",
    "    prune_scores=prune_scores, \n",
    "    grad_func=conf.grad_func,\n",
    "    answer_func=conf.answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    model_out=model_out_test,\n",
    "    complement_circuit_outs=complement_outs_test,\n",
    "    alpha=conf.alpha,\n",
    "    B=1000\n",
    ")\n",
    "\n",
    "save_json(indep_results_test, edge_dir, \"indep_results_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    indep_results_train_eval = independence_tests(\n",
    "        model=task.model, \n",
    "        dataloader=task.train_loader, \n",
    "        prune_scores=prune_scores, \n",
    "        grad_func=conf.eval_grad_func,\n",
    "        answer_func=conf.eval_answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        model_out=model_out_train,\n",
    "        complement_circuit_outs=complement_outs_train,\n",
    "        alpha=conf.alpha,\n",
    "        B=1000\n",
    "    )\n",
    "    save_json(indep_results_train_eval, edge_dir, \"indep_results_train_eval\")\n",
    "\n",
    "    indep_results_test_eval = independence_tests(\n",
    "        model=task.model, \n",
    "        dataloader=task.test_loader, \n",
    "        prune_scores=prune_scores, \n",
    "        grad_func=conf.eval_grad_func,\n",
    "        answer_func=conf.eval_answer_func,\n",
    "        ablation_type=conf.ablation_type,\n",
    "        model_out=model_out_test,\n",
    "        complement_circuit_outs=complement_outs_test,\n",
    "        alpha=conf.alpha,\n",
    "        B=1000\n",
    "    )\n",
    "    save_json(indep_results_test_eval, edge_dir, \"indep_results_test_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot % loss recovered and indep test results\n",
    "fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results_c_train, \n",
    "    indep_results_train,\n",
    "    title=\"(Train) Fraction of Loss Recovered by Complement and Independence Test Results\",\n",
    "    result_type=\"indep\",\n",
    "    x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    ")\n",
    "fig.savefig(edge_dir / \"frac_loss_recovered_and_indep_test_results_train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot % loss recovered and indep test results\n",
    "fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "    faith_metric_results_c_test, \n",
    "    indep_results_test,\n",
    "    title=\"(Test) Fraction of Loss Recovered by Complement and Independence Test Results\",\n",
    "    result_type=\"indep\",\n",
    "    x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    ")\n",
    "fig.savefig(edge_dir / \"frac_loss_recovered_and_indep_test_results_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "        faith_metric_results_c_train_eval, \n",
    "        indep_results_train_eval,\n",
    "        title=\"(Train) Fraction of Loss Recovered by Complement and Independence Test Results\",\n",
    "        result_type=\"indep\",\n",
    "        x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    "    )\n",
    "    fig.savefig(edge_dir / \"frac_loss_recovered_and_indep_test_results_train_eval.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.eval_answer_func is not None and conf.eval_answer_func != conf.answer_func:\n",
    "    fig, ax = plot_frac_loss_recovered_and_equiv_test_results(\n",
    "        faith_metric_results_c_test_eval, \n",
    "        indep_results_test_eval,\n",
    "        title=\"(Test) Fraction of Loss Recovered by Complement and Independence Test Results\",\n",
    "        result_type=\"indep\",\n",
    "        x_label=\"Edges\" if not conf.prune_score_thresh else \"Prune Scores\"\n",
    "    )\n",
    "    fig.savefig(edge_dir / \"frac_loss_recovered_and_indep_test_results_test_eval.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Independence Test on True Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_true_edge_results_path = out_answer_dir / \"indep_true_edge_results.json\"\n",
    "if TASK_TO_OUTPUT_ANSWER_FUNCS[task.key] == (conf.grad_func, conf.answer_func) and not indep_true_edge_results_path.exists():\n",
    "    indep_true_edge_result_test = next(iter(independence_tests(\n",
    "        task.model, \n",
    "        task.test_loader, \n",
    "        task.model.circuit_prune_scores(task.true_edges), \n",
    "        ablation_type=conf.ablation_type,\n",
    "        grad_func=conf.grad_func,\n",
    "        answer_func=conf.answer_func,\n",
    "        thresholds=[0.5], \n",
    "        model_out=model_out_test,\n",
    "        alpha=conf.alpha,\n",
    "        B=1000\n",
    "    ).values()))\n",
    "    save_json(result_to_json(indep_true_edge_result_test), out_answer_dir, f\"indep_true_edge_result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
