{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "import os \n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" #\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Tuple, Dict, Optional\n",
    "import itertools\n",
    "\n",
    "import torch as t\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from auto_circuit.tasks import TASK_DICT\n",
    "from auto_circuit.types import AblationType, PruneScores, BatchKey, Edge, BatchOutputs\n",
    "from auto_circuit_tests.score_funcs import GradFunc, AnswerFunc, DIV_ANSWER_FUNCS\n",
    "from auto_circuit.utils.custom_tqdm import tqdm\n",
    "from auto_circuit.utils.ablation_activations import batch_src_ablations\n",
    "from auto_circuit.prune_algos.activation_patching import compute_loss\n",
    "from auto_circuit.utils.graph_utils import patch_mode, set_all_masks\n",
    "\n",
    "from auto_circuit_tests.utils.utils import RESULTS_DIR\n",
    "from auto_circuit_tests.utils.auto_circuit_utils import set_task_to_single_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config: \n",
    "    task: str = \"Docstring Component Circuit\"\n",
    "    ablation_type: AblationType = AblationType.RESAMPLE\n",
    "    grad_funcs: List[GradFunc] = field(default_factory=lambda: [GradFunc.LOGIT, GradFunc.LOGPROB])\n",
    "    answer_funcs: List[AnswerFunc] = field(default_factory = lambda: [AnswerFunc.MAX_DIFF, AnswerFunc.AVG_VAL, AnswerFunc.KL_DIV, AnswerFunc.JS_DIV])\n",
    "    clean_corrupt: Optional[str] = None\n",
    "    edge_start: Optional[int] = None\n",
    "    edge_range: Optional[int] = None\n",
    "\n",
    "def conf_post_init(conf: Config):\n",
    "    conf.clean_corrupt = \"corrupt\" if conf.ablation_type == AblationType.RESAMPLE else None\n",
    "\n",
    "\n",
    "def get_out_ans_funcs(conf: Config) -> List[Tuple[GradFunc, AnswerFunc]]:\n",
    "    non_div_ans_funcs = [f for f in conf.answer_funcs if f not in DIV_ANSWER_FUNCS]\n",
    "    div_ans_funcs = [f for f in conf.answer_funcs if f in DIV_ANSWER_FUNCS]\n",
    "    return list(itertools.product(conf.grad_funcs, non_div_ans_funcs)) + [(GradFunc.LOGPROB, f) for f in div_ans_funcs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    conf: Config = OmegaConf.merge(OmegaConf.structured(conf), OmegaConf.from_cli(sys.argv[1:]))\n",
    "conf_post_init(conf)\n",
    "out_ans_funcs = get_out_ans_funcs(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dir = RESULTS_DIR / conf.task.replace(\" \", \"_\")\n",
    "ablation_dir = task_dir / conf.ablation_type.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = TASK_DICT[conf.task]\n",
    "# all in one batch b/c no grad\n",
    "set_task_to_single_batch(task)\n",
    "task.shuffle = False\n",
    "task.init_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how do I break this up into smaller chuncks? I guesss just separate into edge ranges\n",
    "# compute and store act patch scores for all combinations of grad_func and answer_func\n",
    "prune_score_dict: Dict[Tuple[GradFunc, AnswerFunc], PruneScores] = {\n",
    "    (grad_func, answer_func): task.model.new_prune_scores()\n",
    "    for grad_func, answer_func in out_ans_funcs\n",
    "}\n",
    "\n",
    "full_model_score_dict: Dict[Tuple[GradFunc, AnswerFunc], PruneScores] = {\n",
    "    (grad_func, answer_func): task.model.new_prune_scores()\n",
    "    for grad_func, answer_func in out_ans_funcs\n",
    "}\n",
    "\n",
    "\n",
    "src_outs: Dict[BatchKey, t.Tensor] = batch_src_ablations(\n",
    "    task.model,\n",
    "    task.train_loader,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    clean_corrupt=conf.clean_corrupt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by seq_idx, src_idx, dest.layer, dest.head\n",
    "edges = sorted(task.model.edges, key=lambda edge: (edge.seq_idx, edge.src.src_idx, edge.dest.layer, edge.dest.head_idx))\n",
    "\n",
    "# if edge_start and edge_range are set, only compute scores for those edges\n",
    "if conf.edge_start is not None and conf.edge_range is not None:\n",
    "    edges = edges[conf.edge_start:min(conf.edge_start + conf.edge_range, len(edges))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute scores on full model\n",
    "model_outs: BatchOutputs = {}\n",
    "with t.no_grad():\n",
    "    for batch in tqdm(task.train_loader, desc=\"Full Model Loss\"):\n",
    "        logits = task.model(batch.clean)[task.model.out_slice]\n",
    "        model_outs[batch.key] = logits.cpu()\n",
    "        for (grad_func, answer_func) in out_ans_funcs:\n",
    "            if answer_func in DIV_ANSWER_FUNCS:\n",
    "                continue\n",
    "            score = -compute_loss(task.model, batch, grad_func.value, answer_func.value, logits=logits)\n",
    "            for mod_name, mod in task.model.patch_masks.items():\n",
    "                full_model_score_dict[(grad_func, answer_func)][mod_name] += score.sum().item()\n",
    "\n",
    "# compute scores for each ablated edge \n",
    "with t.no_grad():\n",
    "    for edge in tqdm(edges, desc=\"Edge Ablation Loss\"):\n",
    "        edge: Edge\n",
    "        set_all_masks(task.model, val=0)\n",
    "        for batch in task.train_loader:\n",
    "            patch_src_outs = src_outs[batch.key].clone().detach()\n",
    "            with patch_mode(task.model, patch_src_outs, edges=[edge]):\n",
    "                logits = task.model(batch.clean)[task.model.out_slice]\n",
    "            for (grad_func, answer_func) in out_ans_funcs:\n",
    "                score = -compute_loss(task.model, batch, grad_func.value, answer_func.value, logits=logits, clean_out=model_outs[batch.key].to(task.device))\n",
    "                full_model_score = full_model_score_dict[(grad_func, answer_func)][edge.dest.module_name][edge.patch_idx]\n",
    "                prune_score_dict[(grad_func, answer_func)][edge.dest.module_name][edge.patch_idx] = full_model_score - score.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out to directories \n",
    "file_postfix = '' if conf.edge_start is None else f'_{conf.edge_start}_{conf.edge_range}'\n",
    "for (grad_func, answer_func) in out_ans_funcs:\n",
    "    score_func_name = f'{grad_func.name}_{answer_func.name}'\n",
    "    ps_path = ablation_dir / score_func_name / f'act_patch_prune_scores{file_postfix}.pt'\n",
    "    ps_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(ps_path)\n",
    "    t.save(prune_score_dict[(grad_func, answer_func)], ps_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-circuit-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
