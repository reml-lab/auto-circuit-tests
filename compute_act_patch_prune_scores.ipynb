{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "import os \n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Tuple, Dict, Optional\n",
    "import itertools\n",
    "\n",
    "import torch as t\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from auto_circuit.tasks import TASK_DICT\n",
    "from auto_circuit.types import AblationType, PruneScores, BatchKey, Edge\n",
    "from auto_circuit_tests.score_funcs import GradFunc, AnswerFunc\n",
    "from auto_circuit.utils.custom_tqdm import tqdm\n",
    "from auto_circuit.utils.ablation_activations import batch_src_ablations\n",
    "from auto_circuit.prune_algos.activation_patching import compute_loss\n",
    "from auto_circuit.utils.graph_utils import patch_mode, set_all_masks\n",
    "\n",
    "from auto_circuit_tests.utils.utils import RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config: \n",
    "    task: str = \"Docstring Component Circuit\"\n",
    "    ablation_type: AblationType = AblationType.RESAMPLE\n",
    "    grad_funcs: List[GradFunc] = field(default_factory=lambda: [GradFunc.LOGIT, GradFunc.LOGPROB])\n",
    "    answer_funcs: List[AnswerFunc] = field(default_factory = lambda: [AnswerFunc.MAX_DIFF, AnswerFunc.AVG_VAL])\n",
    "    clean_corrupt: Optional[str] = None\n",
    "    edge_start: Optional[int] = None\n",
    "    edge_range: Optional[int] = None\n",
    "\n",
    "def conf_post_init(conf: Config):\n",
    "    conf.clean_corrupt = \"corrupt\" if conf.ablation_type == AblationType.RESAMPLE else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config()\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    conf: Config = OmegaConf.merge(OmegaConf.structured(conf), OmegaConf.from_cli(sys.argv[1:]))\n",
    "conf_post_init(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dir = RESULTS_DIR / conf.task.replace(\" \", \"_\")\n",
    "ablation_dir = task_dir / conf.ablation_type.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/transformer_lens/utils.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file_path, map_location=\"cpu\")\n",
      "/nas/ucb/oliveradk/miniforge3/envs/auto-circuit-tests/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model attn-only-4l into HookedTransformer\n",
      "seq_len before divergence None\n",
      "seq_len after divergence None\n"
     ]
    }
   ],
   "source": [
    "task = TASK_DICT[conf.task]\n",
    "# all in one batch b/c no grad\n",
    "task.batch_size = task.batch_size * task.batch_count\n",
    "task.batch_count = 1\n",
    "task.init_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how do I break this up into smaller chuncks? I guesss just separate into edge ranges\n",
    "# compute and store act patch scores for all combinations of grad_func and answer_func\n",
    "prune_score_dict: Dict[Tuple[GradFunc, AnswerFunc], PruneScores] = {\n",
    "    (grad_func, answer_func): task.model.new_prune_scores()\n",
    "    for grad_func, answer_func in itertools.product(conf.grad_funcs, conf.answer_funcs)\n",
    "}\n",
    "\n",
    "src_outs: Dict[BatchKey, t.Tensor] = batch_src_ablations(\n",
    "    task.model,\n",
    "    task.train_loader,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    clean_corrupt=conf.clean_corrupt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by seq_idx, src_idx, dest.layer, dest.head\n",
    "edges = sorted(task.model.edges, key=lambda edge: (edge.seq_idx, edge.src.src_idx, edge.dest.layer, edge.dest.head_idx))\n",
    "\n",
    "# if edge_start and edge_range are set, only compute scores for those edges\n",
    "if conf.edge_start is not None and conf.edge_range is not None:\n",
    "    edges = edges[conf.edge_start:min(conf.edge_start + conf.edge_range, len(edges))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge Ablation Loss██████████| 10/10 [00:02<00:00,  4.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute scores on full model\n",
    "if conf.edge_start is None:\n",
    "    with t.no_grad():\n",
    "        for batch in tqdm(task.train_loader, desc=\"Full Model Loss\"):\n",
    "            logits = task.model(batch.clean)[task.model.out_slice]\n",
    "            for (grad_func, answer_func) in itertools.product(conf.grad_funcs, conf.answer_funcs):\n",
    "                loss = compute_loss(task.model, batch, grad_func.value, answer_func.value, logits=logits)\n",
    "                for mod_name, mod in task.model.patch_masks.items():\n",
    "                    prune_score_dict[(grad_func, answer_func)][mod_name] += loss.sum().item()\n",
    "\n",
    "# compute scores for each ablated edge \n",
    "with t.no_grad():\n",
    "    for edge in tqdm(edges, desc=\"Edge Ablation Loss\"):\n",
    "        edge: Edge\n",
    "        set_all_masks(task.model, val=0)\n",
    "        for batch in task.train_loader:\n",
    "            patch_src_outs = src_outs[batch.key].clone().detach()\n",
    "            with patch_mode(task.model, patch_src_outs, edges=[edge]):\n",
    "                logits = task.model(batch.clean)[task.model.out_slice]\n",
    "            for (grad_func, answer_func) in itertools.product(conf.grad_funcs, conf.answer_funcs):\n",
    "                loss = compute_loss(task.model, batch, grad_func.value, answer_func.value, logits=logits)\n",
    "                prune_score_dict[(grad_func, answer_func)][edge.dest.module_name][edge.patch_idx] -= loss.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Component_Circuit/RESAMPLE/LOGIT_MAX_DIFF/act_patch_prune_scores_0_10.pkl\n",
      "/nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Component_Circuit/RESAMPLE/LOGIT_AVG_VAL/act_patch_prune_scores_0_10.pkl\n",
      "/nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Component_Circuit/RESAMPLE/LOGPROB_MAX_DIFF/act_patch_prune_scores_0_10.pkl\n",
      "/nas/ucb/oliveradk/auto-circuit-tests/output/hypo_test_results/Docstring_Component_Circuit/RESAMPLE/LOGPROB_AVG_VAL/act_patch_prune_scores_0_10.pkl\n"
     ]
    }
   ],
   "source": [
    "# save out to directories \n",
    "file_postfix = '' if conf.edge_start is None else f'_{conf.edge_start}_{conf.edge_range}'\n",
    "for (grad_func, answer_func) in itertools.product(conf.grad_funcs, conf.answer_funcs):\n",
    "    score_func_name = f'{grad_func.name}_{answer_func.name}'\n",
    "    ps_path = ablation_dir / score_func_name / f'act_patch_prune_scores{file_postfix}.pkl'\n",
    "    print(ps_path)\n",
    "    t.save(prune_score_dict[(grad_func, answer_func)], ps_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-circuit-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
